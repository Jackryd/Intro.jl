{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Transformers in Julia\n",
    "This notebook presents an introduction to Transformers in Julia. The following code contains a toy model example demonstrating a **very** simple transformer and also one slightly more optimised and improved for better performance. I also present a simple training script which uses an autoregressive training scheme to learn a model to predict how to generate the next amino acid in an antibody sequence. I then also demonstrate how you can perform sampling by a simple greedy sampling loop which iteratively adds token to an antibody sequence. Note that to run the antibody training code you will need to download some sequences from [here](https://opig.stats.ox.ac.uk/webapps/oas/oas_unpaired/) or you can try training on something else!\n",
    "\n",
    "I hope that if you're interested in learning how to work with transformers this notebook can serve as a good initial introduction. If you do however want to deepen your understanding I recommend some of the following resources:\n",
    "- 3Blue1Brown's Transformer [video series](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "- Andrej Karpathy's [youtube videos](https://www.youtube.com/@AndrejKarpathy)\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "- If you want the deepest understanding I also recommend reading some of the hallmark paper's which have layed the groundwork for transformers, like *Attention is All You Need*\n",
    "- Also, looking through the [Onion.jl](https://github.com/MurrellGroup/Onion.jl/tree/main) or [Transformers.jl](https://github.com/chengchingwen/Transformers.jl) can be a good way to spend some time\n",
    "- You can also always email me at: jack.collier.ryder@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Our First Toy Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this model we use the Attention and RoPE functions from the Onion.jl package\n",
    "# Flux will be used later for the training and the dense layers in the model also come from there\n",
    "using Flux\n",
    "using Onion\n",
    "using Onion: Attention, causal_mask\n",
    "\n",
    "# This transformer layer is the layer which contains the feedforward and attention parts of the code: the model's processing part\n",
    "# We create a struct which consists of the attention layer (from Onion.jl) and a feedforward network which we say is just a chain of 2 dense layers\n",
    "struct TransformerLayer\n",
    "    attention::Attention\n",
    "    ff::Chain\n",
    "end\n",
    "\n",
    "# This line is important to make all of the parameters trainable!\n",
    "Flux.@layer TransformerLayer\n",
    "\n",
    "# Constructor function for our transformer layers\n",
    "function TransformerLayer(dim::Int, n_head::Int)\n",
    "    return TransformerLayer(\n",
    "        Attention(dim, n_head),\n",
    "        Chain(Dense(dim, 4dim, relu), Dense(4dim, dim))\n",
    "    )\n",
    "end\n",
    "\n",
    "# The actual forward pass of our transformer layer\n",
    "function (layer::TransformerLayer)(x::AbstractArray)\n",
    "    # These first four lines are used to create an attention mask and to prepare our positional encoding\n",
    "    # Our attention mask is a causal mask which is a mask which makes it so that each token in the sequence can only attend to the tokens before itself\n",
    "    # This is important if we don't want data leakage since we're training a model to predict the next token!\n",
    "    seqlen = size(x, 2)\n",
    "    h = zeros(Float32, layer.attention.dim, seqlen, 1)\n",
    "    mask = causal_mask(h)\n",
    "    # The RoPE is the positional encoding that I've decided to use. This is a more modern type of positional encdoing (it's from like 2021, but remember that the og transformer is from 2017..)\n",
    "    # which uses a rotation matrix to encode the position of the tokens in the sequence. Without this the model won't ever know what position a token is at in a sequence, and this can obviously become problematic!\n",
    "    # Consider language just!\n",
    "    rope = RoPE(layer.attention.head_dim, seqlen)\n",
    "\n",
    "    # These two layers then perform the attention and then return the input after having passed it through that feedforward network!\n",
    "    x = layer.attention(x, 1, rope, mask)\n",
    "    return layer.ff(x)\n",
    "end\n",
    "\n",
    "# This is our struct for our transformer model core\n",
    "# Here we also specify what layers we want in the model; for this toy example we have\n",
    "# - The input embedding (here called vocab_embed)\n",
    "# - A vector of TransformerLayers\n",
    "# - An output projection\n",
    "struct ToyModel\n",
    "    vocab_embed::Dense\n",
    "    layers::Vector{TransformerLayer}\n",
    "    proj::Dense\n",
    "end\n",
    "\n",
    "# To make our parameters trainable!\n",
    "Flux.@layer ToyModel\n",
    "\n",
    "# Constructor function for our model\n",
    "function ToyModel(layers::Int, dim::Int, n_head::Int)\n",
    "    vocab_embed = Dense(vocab_size, dim, bias=false)\n",
    "    layers = [TransformerLayer(dim, n_head) for _ in 1:layers]\n",
    "    proj = Dense(dim, vocab_size)\n",
    "    return ToyModel(vocab_embed, layers, proj)\n",
    "end\n",
    "\n",
    "# Here is our forward pass for the model\n",
    "function (model::ToyModel)(x)\n",
    "    # First we embed our sequences into that high-dim vector space \n",
    "    x = model.vocab_embed(x)\n",
    "    # We then pass our x through all of these transformer layers. I.e. we're applying attention and a feedforward network multiple times\n",
    "    for layer in model.layers\n",
    "        x = layer(x)\n",
    "    end\n",
    "    # We then return a projection of our sequence and as we see in our constructor function this is a layer which goes from dim -> vocabs size, so we're basically mapping\n",
    "    # frm this high dimensional vector space to a vector which has vocab_size dimensions - i.e. we're basically getting a probability distributions over all of our tokens!\n",
    "    return model.proj(x)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling BangBangDataFramesExt [d787bcad-b5c5-56bb-adaa-6bfddb178a59]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling TransducersDataFramesExt [cefb4096-3352-5e5f-8501-71f024082a88]\n"
     ]
    }
   ],
   "source": [
    "using CSV, DataFrames, OneHotArrays\n",
    "\n",
    "# I load in some sequences from a csv file and I pad them to a length of 128\n",
    "df = CSV.File(\"SRR14611332_1_Heavy_IGHM.csv\", header=2) |> DataFrame\n",
    "max_len = 128\n",
    "seqs = df[!, \"sequence_alignment_aa\"]\n",
    "seqs = seqs[length.(seqs) .< max_len]\n",
    "seqs = \"#\" .* rpad.(seqs, max_len, '-')\n",
    "alphabet = collect(\"-ACDEFGHIKLMNPQRSTVWY#\");\n",
    "vocab_size = length(alphabet)\n",
    "prep_batch(seqs, alphabet) = stack(onehotbatch.(seqs, (alphabet,)))\n",
    "\n",
    "function create_batches(seqs, alphabet; batch_size=4)\n",
    "    seq_batches = [collect(batch) for batch in Iterators.partition(seqs, batch_size)]\n",
    "    onehot_batches = [prep_batch(batch, alphabet) for batch in seq_batches]\n",
    "    return onehot_batches\n",
    "end\n",
    "\n",
    "# I then create one hot encoded batches of said sequences\n",
    "batches = create_batches(seqs, alphabet);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(vocab_embed = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), layers = @NamedTuple{attention::@NamedTuple{wq::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, wk::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, wv::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, wo::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, dim::Tuple{}, n_heads::Tuple{}, n_kv_heads::Tuple{}, head_dim::Tuple{}}, ff::@NamedTuple{layers::Tuple{@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, σ::Tuple{}}, @NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, σ::Tuple{}}}}}[(attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),)), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),)), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),)), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),)), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),)), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),)), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),)), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),))], proj = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I create an optimiser and initiate a model\n",
    "using Optimisers\n",
    "layers = 8\n",
    "dim = 128\n",
    "n_heads = 2\n",
    " \n",
    "model = ToyModel(layers, dim, n_heads)\n",
    "base_lr = 0.001f0\n",
    "rule = Optimisers.Adam(base_lr) \n",
    "opt_state = Optimisers.setup(rule, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((vocab_embed = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-2.93262f-20 5.13471f-9 … 1.00289f-10 -1.25433f-5; -5.13023f-20 -9.63508f-10 … -3.51995f-11 -1.29556f-5; … ; -3.00676f-20 -3.28148f-9 … -2.65414f-11 2.87152f-5; 2.07229f-19 -1.0479f-9 … 1.55479f-10 -2.22726f-5], Float32[8.6002f-41 2.63649f-18 … 1.00578f-21 1.57332f-11; 2.63189f-40 9.28335f-20 … 1.23899f-22 1.67847f-11; … ; 9.0405f-41 1.0768f-18 … 7.04439f-23 8.24549f-11; 4.29433f-39 1.09809f-19 … 2.41733f-21 4.96063f-11], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), layers = @NamedTuple{attention::@NamedTuple{wq::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, wk::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, wv::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, wo::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, dim::Tuple{}, n_heads::Tuple{}, n_kv_heads::Tuple{}, head_dim::Tuple{}}, ff::@NamedTuple{layers::Tuple{@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, σ::Tuple{}}, @NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, σ::Tuple{}}}}}[(attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[2.69703f-9 -6.73052f-10 … 7.22546f-10 -2.57083f-9; 9.76107f-11 2.68393f-10 … 2.71667f-10 -1.9199f-10; … ; 1.8241f-9 3.51269f-9 … 4.09236f-10 -6.78304f-10; 2.68318f-9 -3.29017f-9 … -1.07154f-9 -2.46441f-9], Float32[7.27386f-19 4.52993f-20 … 5.22066f-20 6.60907f-19; 9.52772f-22 7.2034f-21 … 7.38022f-21 3.68598f-21; … ; 3.32731f-19 1.23388f-18 … 1.67472f-20 4.6009f-20; 7.19937f-19 1.08251f-18 … 1.14818f-19 6.07324f-19], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[7.34841f-10 -9.83154f-10 … -1.89045f-10 -1.47285f-9; 1.61333f-9 -2.4748f-9 … -1.19331f-9 -2.61331f-9; … ; -2.60969f-9 3.89313f-9 … 1.78736f-9 4.35813f-9; -1.39577f-9 5.60197f-10 … 2.91798f-10 1.54338f-9], Float32[5.39985f-20 9.66578f-20 … 3.57377f-21 2.16927f-19; 2.6028f-19 6.12456f-19 … 1.42398f-19 6.82929f-19; … ; 6.81041f-19 1.51563f-18 … 3.19462f-19 1.89931f-18; 1.94814f-19 3.13816f-20 … 8.51447f-21 2.38198f-19], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[3.15892f-7 5.12043f-6 … 1.95469f-6 9.19232f-6; -2.11518f-7 -1.09009f-6 … -3.70295f-7 -1.81703f-6; … ; -2.26263f-7 -9.73147f-7 … -3.31314f-7 -1.72745f-6; -4.63565f-8 -1.17155f-6 … -3.83843f-7 -2.05869f-6], Float32[9.97863f-15 2.62184f-12 … 3.82075f-13 8.44975f-12; 4.47394f-15 1.18828f-13 … 1.37116f-14 3.30155f-13; … ; 5.11945f-15 9.47002f-14 … 1.09768f-14 2.98405f-13; 2.14889f-16 1.37251f-13 … 1.47334f-14 4.23814f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[6.3699f-7 7.63867f-7 … 4.93682f-7 -1.30796f-6; -1.48868f-6 -4.63739f-6 … -1.90152f-6 6.5349f-6; … ; -6.96356f-7 -7.2958f-7 … -4.90939f-7 1.42493f-6; 3.87301f-6 8.58876f-6 … 4.03295f-6 -1.29933f-5], Float32[4.0575f-14 5.83486f-14 … 2.43719f-14 1.71073f-13; 2.21613f-13 2.15051f-12 … 3.61573f-13 4.27043f-12; … ; 4.84905f-14 5.3228f-14 … 2.41018f-14 2.0304f-13; 1.5f-12 7.37658f-12 … 1.62645f-12 1.68824f-11], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 1.0661f-7 7.25388f-8 … 2.73141f-7 -2.41887f-7; … ; 5.84647f-8 5.00227f-8 … 1.37314f-7 -1.09786f-7; -4.88243f-8 -1.31084f-7 … -2.77208f-7 1.08481f-7], Float32[0.0 0.0 … 0.0 0.0; 1.13656f-15 5.26181f-16 … 7.46051f-15 5.85083f-15; … ; 3.41808f-16 2.50223f-16 … 1.88548f-15 1.20529f-15; 2.38378f-16 1.71827f-15 … 7.68432f-15 1.1768f-15], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 2.61276f-6, 0.0, -2.00684f-5, 1.06338f-5, -1.62646f-5, 1.96422f-5, -5.60411f-7, 1.71831f-9, 3.55859f-10  …  -1.22077f-5, 8.31491f-10, 1.41956f-8, 9.84185f-9, -1.43557f-6, -5.03276f-6, 3.04449f-5, 1.13683f-6, 1.18695f-6, -2.80221f-6], Float32[0.0, 6.8264f-13, 0.0, 4.02734f-11, 1.13076f-11, 2.64535f-11, 3.85809f-11, 3.14057f-14, 2.95255f-19, 1.26634f-20  …  1.49027f-11, 6.91368f-20, 2.01512f-17, 9.68607f-18, 2.06083f-13, 2.53284f-12, 9.26882f-11, 1.29237f-13, 1.40883f-13, 7.85226f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 -1.48683f-9 … -1.81457f-8 -3.16029f-8; 0.0 1.21377f-8 … 6.89576f-9 3.67262f-8; … ; 0.0 1.24299f-8 … 9.91227f-10 -5.43795f-9; 0.0 3.53569f-8 … 8.32139f-9 -3.35958f-9], Float32[0.0 2.21062f-19 … 3.29263f-17 9.98727f-17; 0.0 1.47322f-17 … 4.75508f-18 1.3488f-16; … ; 0.0 1.54499f-17 … 9.82518f-20 2.95709f-18; 0.0 1.25009f-16 … 6.92446f-18 1.12866f-18], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[1.34607f-5, 2.89825f-5, -1.31784f-5, 3.08383f-5, -9.37122f-6, -1.75903f-5, 3.79407f-6, 1.39654f-5, -2.61397f-5, 1.69039f-6  …  -7.3034f-5, 7.28613f-6, -1.35829f-5, 4.1308f-5, 2.87549f-5, -1.26326f-5, -4.76567f-5, -4.28994f-6, 2.08386f-5, 3.79673f-5], Float32[1.81189f-11, 8.39974f-11, 1.73668f-11, 9.50991f-11, 8.78185f-12, 3.09416f-11, 1.43948f-12, 1.95029f-11, 6.83273f-11, 2.85736f-13  …  5.33389f-10, 5.3087f-12, 1.84492f-11, 1.70633f-10, 8.26831f-11, 1.5958f-11, 2.27113f-10, 1.84033f-12, 4.34241f-11, 1.44149f-10], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ())),)), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-1.30575f-9 2.09129f-10 … 8.92822f-10 -3.66656f-11; 9.18805f-10 -7.93151f-11 … -6.77803f-10 -8.13296f-12; … ; 6.71957f-10 -9.11702f-11 … -5.04422f-10 6.06506f-11; 1.065f-10 -2.34408f-11 … -6.65021f-11 2.76234f-11], Float32[1.70495f-19 4.37345f-21 … 7.97121f-20 1.34435f-22; 8.44191f-20 6.2908f-22 … 4.59411f-20 6.61442f-24; … ; 4.51521f-20 8.3119f-22 … 2.54438f-20 3.67845f-22; 1.1342f-21 5.49465f-23 … 4.42247f-22 7.63041f-23], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-6.15881f-10 1.08264f-9 … 2.50034f-10 5.14639f-10; -1.64509f-9 9.97684f-10 … 1.02141f-9 5.28854f-10; … ; 3.81304f-10 -4.8445f-10 … -1.50656f-10 -2.67126f-10; -4.23644f-10 5.29816f-10 … 1.95f-10 2.92859f-10], Float32[3.79304f-20 1.17209f-19 … 6.25161f-21 2.6485f-20; 2.70629f-19 9.9536f-20 … 1.04326f-19 2.79683f-20; … ; 1.4539f-20 2.34689f-20 … 2.26969f-21 7.13554f-21; 1.79472f-20 2.80701f-20 … 3.80247f-21 8.5765f-21], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-9.47154f-6 5.73513f-6 … 5.7216f-6 2.14472f-6; 4.68358f-6 -2.63349f-6 … -2.90534f-6 -1.00481f-6; … ; 2.04651f-8 3.18394f-8 … -6.50457f-8 -9.59531f-8; -9.09389f-6 4.76446f-6 … 5.73423f-6 1.76032f-6], Float32[8.97088f-12 3.28913f-12 … 3.27362f-12 4.59976f-13; 2.19357f-12 6.93517f-13 … 8.44091f-13 1.00964f-13; … ; 4.18813f-17 1.01373f-16 … 4.23088f-16 9.20687f-16; 8.26978f-12 2.26998f-12 … 3.2881f-12 3.09868f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-9.21157f-8 -1.74587f-6 … 1.26938f-7 -2.87582f-7; 6.32941f-7 3.45245f-6 … 5.92585f-7 -5.38341f-7; … ; 1.00595f-7 3.42309f-8 … 1.63445f-7 -2.27361f-7; -4.72896f-8 -5.08173f-7 … -1.64269f-7 5.89365f-8], Float32[8.4852f-16 3.04801f-13 … 1.61131f-15 8.27024f-15; 4.00609f-14 1.19192f-12 … 3.51153f-14 2.89807f-14; … ; 1.01193f-15 1.17174f-16 … 2.67138f-15 5.16925f-15; 2.23627f-16 2.58236f-14 … 2.6984f-15 3.47346f-16], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; -1.3539f-8 1.01868f-7 … 8.02873f-8 -2.04332f-8], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 1.83303f-17 1.0377f-15 … 6.44597f-16 4.17512f-17], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, -5.63551f-7, 3.15688f-5, 1.5687f-5, -6.99579f-6, -4.2736f-7, 0.0, 4.77323f-5, 3.82199f-6  …  0.0, 0.0, -2.58111f-5, -5.10721f-7, 8.42839f-6, 1.93895f-6, 0.0, 9.04586f-6, 0.0, -1.89329f-6], Float32[0.0, 0.0, 3.17585f-14, 9.96578f-11, 2.46079f-11, 4.89405f-12, 1.82634f-14, 0.0, 2.27835f-10, 1.46074f-12  …  0.0, 0.0, 6.66201f-11, 2.60832f-14, 7.10368f-12, 3.75946f-13, 0.0, 8.18264f-12, 0.0, 3.58448f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 2.65364f-7; 0.0 0.0 … 0.0 -1.23607f-7; … ; 0.0 0.0 … 0.0 -9.6341f-8; 0.0 0.0 … 0.0 -3.98473f-8], Float32[0.0 0.0 … 0.0 7.04172f-15; 0.0 0.0 … 0.0 1.52786f-15; … ; 0.0 0.0 … 0.0 9.28147f-16; 0.0 0.0 … 0.0 1.58779f-16], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.000122149, -4.63466f-5, -2.95384f-5, 3.58804f-5, 1.74603f-5, 6.74792f-5, -5.3813f-5, 0.000176433, -4.17662f-5, -0.000101019  …  9.47693f-5, 4.30201f-5, -0.000131254, -1.04272f-5, 0.000143902, -6.87643f-6, -1.40323f-5, 0.000114039, -2.92423f-5, -7.2027f-6], Float32[1.49202f-9, 2.14798f-10, 8.72505f-11, 1.28738f-10, 3.04857f-11, 4.55339f-10, 2.8958f-10, 3.11282f-9, 1.74439f-10, 1.02047f-9  …  8.98111f-10, 1.8507f-10, 1.72273f-9, 1.08726f-11, 2.07075f-9, 4.72847f-12, 1.96904f-11, 1.30047f-9, 8.55101f-11, 5.18783f-12], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ())),)), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[1.09455f-11 -1.49543f-11 … 1.75677f-10 -1.04036f-10; -6.64517f-12 -7.72404f-13 … -6.57023f-11 4.09395f-11; … ; 7.65033f-12 -9.12426f-12 … 3.99554f-11 -2.09872f-11; 8.66838f-12 -1.0259f-11 … 4.82359f-11 -2.58322f-11], Float32[1.19802f-23 2.23629f-23 … 3.08619f-21 1.08233f-21; 4.41576f-24 5.966f-26 … 4.31673f-22 1.67602f-22; … ; 5.85268f-24 8.32509f-24 … 1.59641f-22 4.40456f-23; 7.51398f-24 1.05246f-23 … 2.32667f-22 6.67296f-23], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[5.75532f-12 -3.66148f-11 … -9.91792f-11 1.08921f-10; 5.58481f-12 -2.10994f-11 … -1.17396f-10 1.06569f-10; … ; 2.63328f-12 -6.08654f-12 … -4.71914f-12 1.50997f-11; 3.0039f-13 -1.37931f-12 … -6.72658f-13 2.81703f-12], Float32[3.31232f-24 1.34062f-22 … 9.83639f-22 1.18637f-21; 3.11897f-24 4.45181f-23 … 1.37817f-21 1.13567f-21; … ; 6.93408f-25 3.70455f-24 … 2.227f-24 2.27999f-23; 9.0233f-27 1.90248f-25 … 4.52463f-26 7.93554f-25], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[9.86932f-8 -6.64029f-7 … -1.86351f-6 2.16484f-6; 4.84742f-8 -1.84465f-7 … -9.02293f-7 9.16935f-7; … ; 4.33836f-8 1.0369f-7 … 7.94252f-7 -7.30759f-7; -1.21384f-7 1.37707f-6 … 6.19672f-6 -6.39769f-6], Float32[9.74022f-16 4.40929f-14 … 3.47261f-13 4.68647f-13; 2.34972f-16 3.40268f-15 … 8.14121f-14 8.40759f-14; … ; 1.88211f-16 1.07515f-15 … 6.30827f-14 5.34002f-14; 1.47338f-15 1.8963f-13 … 3.83989f-12 4.09299f-12], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-6.81187f-9 -5.27861f-9 … -3.91099f-8 -1.04055f-7; -1.45376f-8 -2.79914f-8 … -3.48151f-8 -1.65411f-8; … ; -7.83892f-7 -2.24099f-6 … -4.40125f-7 4.86296f-6; 6.88348f-7 2.21291f-6 … 2.41595f-7 -4.98242f-6], Float32[4.6401f-18 2.78634f-18 … 1.52957f-16 1.08272f-15; 2.11338f-17 7.83507f-17 … 1.21207f-16 2.73604f-17; … ; 6.14478f-14 5.02196f-13 … 1.93707f-14 2.36481f-12; 4.73817f-14 4.89689f-13 … 5.83673f-15 2.48241f-12], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[2.07915f-9 -2.64553f-10 … 4.18953f-11 -4.98891f-10; -6.05112f-8 -1.81537f-8 … 8.28186f-9 2.23162f-8; … ; 1.64487f-6 -2.35948f-6 … 4.14373f-6 2.92359f-6; 1.46767f-6 -1.77945f-6 … 3.10868f-6 2.17381f-6], Float32[4.32279f-19 6.99875f-21 … 1.75519f-22 2.48889f-20; 3.66156f-16 3.29551f-17 … 6.85882f-18 4.98005f-17; … ; 2.70556f-13 5.56706f-13 … 1.71703f-12 8.54729f-13; 2.15402f-13 3.16641f-13 … 9.66378f-13 4.7254f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-6.37978f-8, 1.61819f-6, 0.0, -5.11019f-6, 0.0, -2.75833f-6, -7.37527f-8, -1.5613f-5, 0.0, -6.5738f-5  …  -1.45414f-6, -7.5815f-6, 9.04901f-8, -8.68259f-5, -2.90924f-5, 2.61061f-5, -1.3958f-7, 3.43272f-6, -8.81908f-5, -7.31214f-5], Float32[4.07011f-16, 2.61849f-13, 0.0, 2.61137f-12, 0.0, 7.60826f-13, 5.43938f-16, 2.43764f-11, 0.0, 4.32143f-10  …  2.11451f-13, 5.74784f-12, 8.18834f-16, 7.53863f-10, 8.46354f-11, 6.81519f-11, 1.94822f-15, 1.17834f-12, 7.77751f-10, 5.34666f-10], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-9.60456f-11 -4.30711f-6 … -3.86213f-6 -5.18597f-6; 1.77781f-10 2.23891f-6 … 1.98731f-6 2.59951f-6; … ; -1.52424f-9 -6.46467f-6 … -5.84407f-6 -7.9549f-6; 4.95903f-10 -5.33732f-7 … -4.49649f-7 -6.82045f-7], Float32[9.22463f-22 1.85509f-12 … 1.49158f-12 2.68939f-12; 3.16056f-21 5.01266f-13 … 3.94936f-13 6.75734f-13; … ; 2.32327f-19 4.17914f-12 … 3.41527f-12 6.32796f-12; 2.45917f-20 2.84866f-14 … 2.02181f-14 4.65179f-14], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-0.000113632, 5.41854f-5, -0.000183206, 3.16623f-7, -4.18021f-5, -6.39496f-5, 5.47099f-5, -0.000168745, 0.000129318, -2.6763f-5  …  -7.04356f-5, -8.46362f-5, 5.63377f-5, 0.000192127, 8.66693f-5, 5.63641f-5, 5.70108f-5, 7.96481f-5, -0.000180395, -1.20746f-5], Float32[1.2912f-9, 2.93601f-10, 3.35641f-9, 1.00249f-14, 1.74739f-10, 4.0895f-10, 2.99314f-10, 2.84745f-9, 1.67229f-9, 7.16247f-11  …  4.96111f-10, 7.1632f-10, 3.17389f-10, 3.69123f-9, 7.51147f-10, 3.17687f-10, 3.25018f-10, 6.34374f-10, 3.25421f-9, 1.45794f-11], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ())),)), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-3.51898f-12 -2.10668f-12 … 4.27744f-13 1.78034f-12; 5.05734f-12 3.17502f-12 … -7.01307f-13 -2.63213f-12; … ; -2.78154f-12 -1.73446f-12 … 6.16351f-13 9.83563f-13; 1.24559f-11 8.90603f-12 … -3.42442f-12 -5.40432f-12], Float32[1.23831f-24 4.43806f-25 … 1.82963f-26 3.16955f-25; 2.55763f-24 1.00806f-24 … 4.91824f-26 6.92802f-25; … ; 7.73684f-25 3.00831f-25 … 3.79884f-26 9.67382f-26; 1.55148f-23 7.93163f-24 … 1.17265f-24 2.92062f-24], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-1.98878f-11 -1.00044f-11 … 1.64706f-12 1.13375f-11; 2.30274f-11 9.8274f-12 … -8.70255f-13 -1.47692f-11; … ; -2.50655f-11 1.23031f-12 … -1.06867f-11 2.40468f-11; -4.1979f-11 1.9906f-12 … -1.7876f-11 4.17763f-11], Float32[3.95519f-23 1.00088f-23 … 2.71277f-25 1.28537f-23; 5.30255f-23 9.65764f-24 … 7.57334f-26 2.18128f-23; … ; 6.2827f-23 1.51363f-25 … 1.14203f-23 5.7824f-23; 1.76222f-22 3.96243f-25 … 3.19547f-23 1.74524f-22], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[1.34854f-6 4.21392f-7 … 9.60842f-8 -9.23237f-7; -5.5466f-6 -2.72183f-6 … 4.96618f-7 3.14432f-6; … ; -3.74048f-6 -1.89695f-6 … 3.75521f-7 2.10147f-6; -2.78628f-6 -1.40639f-6 … 2.9296f-7 1.52632f-6], Float32[1.81853f-13 1.77569f-14 … 9.23204f-16 8.52355f-14; 3.07644f-12 7.40828f-13 … 2.46626f-14 9.88665f-13; … ; 1.3991f-12 3.59838f-13 … 1.41014f-14 4.41613f-13; 7.76324f-13 1.9779f-13 … 8.58246f-15 2.32962f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-1.21205f-7 -8.46174f-8 … 2.18903f-7 8.4606f-7; 3.61214f-7 -1.31536f-7 … -3.94676f-7 -1.19921f-6; … ; -4.09923f-7 5.9747f-8 … 4.6976f-7 1.754f-6; -1.86971f-6 2.37719f-7 … 2.44638f-6 7.94926f-6], Float32[1.46903f-15 7.16f-16 … 4.79179f-15 7.15809f-14; 1.30474f-14 1.73015f-15 … 1.55767f-14 1.43808f-13; … ; 1.68035f-14 3.56965f-16 … 2.20672f-14 3.07649f-13; 3.49576f-13 5.65097f-15 … 5.98471f-13 6.31899f-12], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-6.591f-7 1.24034f-6 … -9.33153f-7 1.70072f-6; -4.13563f-9 1.96797f-8 … -2.00408f-8 2.09978f-8; … ; 0.0 0.0 … 0.0 0.0; 1.19828f-6 -2.28979f-6 … 1.66813f-6 -2.91101f-6], Float32[4.34407f-14 1.53842f-13 … 8.70763f-14 2.89242f-13; 1.71032f-18 3.87286f-17 … 4.0163f-17 4.40904f-17; … ; 0.0 0.0 … 0.0 0.0; 1.43586f-13 5.24308f-13 … 2.78263f-13 8.47385f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-5.07814f-5, -1.13585f-6, 0.000204698, -8.70424f-5, 0.000174684, -0.00042729, 0.0, 0.0, 0.0, -3.91428f-5  …  0.000141169, 2.05954f-5, 7.28352f-6, 0.0, -0.000115686, 4.32668f-7, 0.0, 0.0, 0.0, 9.53941f-5], Float32[2.57872f-10, 1.29013f-13, 4.19005f-9, 7.57627f-10, 3.0514f-9, 1.82574f-8, 0.0, 0.0, 0.0, 1.53214f-10  …  1.99284f-9, 4.24166f-11, 5.3049f-12, 0.0, 1.3383f-9, 1.87199f-14, 0.0, 0.0, 0.0, 9.09991f-10], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[1.31728f-7 5.5444f-9 … 0.0 2.82461f-7; 1.48257f-7 5.47437f-9 … 0.0 4.26139f-7; … ; 9.34358f-8 3.76942f-10 … 0.0 2.23867f-7; -1.19122f-7 -2.16788f-8 … 0.0 -2.27674f-7], Float32[1.73521f-15 3.07399f-18 … 0.0 7.9783f-15; 2.19798f-15 2.99683f-18 … 0.0 1.81592f-14; … ; 8.73013f-16 1.42084f-20 … 0.0 5.01158f-15; 1.41899f-15 4.69966f-17 … 0.0 5.18347f-15], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.000306039, 0.000371902, 0.000304909, -0.000338965, 0.000184794, -0.000235132, -0.000395932, 0.000148977, -0.000171203, 0.000193999  …  0.000526888, 0.000108436, -0.000245397, 5.15384f-5, 0.000207308, 0.000243911, -0.000239886, 0.000230958, 0.000210773, -0.000274663], Float32[9.36588f-9, 1.3831f-8, 9.29681f-9, 1.14896f-8, 3.41482f-9, 5.52864f-9, 1.5676f-8, 2.21938f-9, 2.931f-9, 3.7635f-9  …  2.77608f-8, 1.17582f-9, 6.02187f-9, 2.65617f-10, 4.29761f-9, 5.94917f-9, 5.75446f-9, 5.33411f-9, 4.44248f-9, 7.54386f-9], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ())),)), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-1.43408f-13 4.76522f-13 … 3.88264f-13 9.04606f-13; 2.59193f-13 1.64916f-12 … 1.38285f-12 1.67745f-12; … ; -4.9591f-13 -3.28725f-12 … -8.1437f-13 -3.1432f-12; -7.29637f-13 -5.37752f-12 … -6.79844f-13 -5.14042f-12], Float32[2.05657f-27 2.27071f-26 … 1.50747f-26 8.18302f-26; 6.71799f-27 2.71969f-25 … 1.91224f-25 2.81381f-25; … ; 2.45924f-26 1.08058f-24 … 6.6319f-26 9.87957f-25; 5.32363f-26 2.89173f-24 … 4.62182f-26 2.64236f-24], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-1.51595f-13 -2.8822f-12 … -2.54079f-12 -3.53459f-12; 1.65654f-13 1.52081f-12 … 1.49599f-12 1.73962f-12; … ; -5.55596f-14 6.07204f-14 … -1.08449f-12 -7.96813f-15; 3.46423f-13 8.63577f-13 … -7.31801f-12 -6.55601f-13], Float32[2.29808f-27 8.30694f-25 … 6.45554f-25 1.24932f-24; 2.74409f-27 2.31283f-25 … 2.23795f-25 3.02623f-25; … ; 3.08682f-28 3.68692f-28 … 1.17611f-25 6.34902f-30; 1.20007f-26 7.45754f-26 … 5.35525f-24 4.29807f-26], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[4.55419f-7 4.31335f-6 … 3.54413f-6 4.85435f-6; 3.763f-7 2.41688f-6 … 1.78232f-6 2.46671f-6; … ; -5.64918f-7 -4.30447f-6 … -3.1913f-6 -4.58567f-6; 7.96308f-7 6.42036f-6 … 4.6929f-6 6.92424f-6], Float32[2.07403f-14 1.86047f-12 … 1.25607f-12 2.35644f-12; 1.416f-14 5.84124f-13 … 3.17661f-13 6.08456f-13; … ; 3.19128f-14 1.85282f-12 … 1.01842f-12 2.10281f-12; 6.34098f-14 4.12204f-12 … 2.2023f-12 4.79444f-12], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[2.40128f-7 -4.2439f-7 … -3.03622f-7 3.66256f-7; 1.9942f-6 -3.31193f-6 … -3.25257f-6 2.94863f-6; … ; 4.63956f-6 -7.43783f-6 … -7.23465f-6 6.63204f-6; -6.09546f-7 9.83109f-7 … 8.92507f-7 -8.61215f-7], Float32[5.76605f-15 1.80104f-14 … 9.21852f-15 1.34142f-14; 3.97679f-13 1.09687f-12 … 1.05791f-12 8.69429f-13; … ; 2.15252f-12 5.53205f-12 … 5.23395f-12 4.39833f-12; 3.71541f-14 9.6649f-14 … 7.96558f-14 7.41682f-14], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[2.29361f-7 8.57302f-8 … 2.46635f-7 -4.04785f-7; -1.85393f-6 -1.43228f-6 … -2.26651f-6 3.01568f-6; … ; 2.82195f-6 2.12218f-6 … 3.43025f-6 -4.57478f-6; -1.27093f-6 -9.39924f-7 … -1.53919f-6 2.04669f-6], Float32[5.26057f-15 7.34956f-16 … 6.0828f-15 1.63849f-14; 3.437f-13 2.05139f-13 … 5.13701f-13 9.09423f-13; … ; 7.96331f-13 4.5036f-13 … 1.17664f-12 2.09283f-12; 1.61524f-13 8.83445f-14 … 2.36907f-13 4.18888f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-3.03038f-5, 0.000290842, -0.000102107, 0.0, -7.05389f-5, 6.71714f-5, 6.25531f-5, -0.000198981, 0.000371022, 0.0  …  4.73062f-5, 0.0, 0.0, 0.0, -0.000333357, -3.80213f-5, 0.0, 1.59562f-7, -0.00044035, 0.000198302], Float32[9.18308f-11, 8.45879f-9, 1.04256f-9, 0.0, 4.97566f-10, 4.51193f-10, 3.91284f-10, 3.95929f-9, 1.37656f-8, 0.0  …  2.23784f-10, 0.0, 0.0, 0.0, 1.11125f-8, 1.4456f-10, 0.0, 2.54596f-15, 1.93906f-8, 3.93234f-9], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-2.05482f-6 -2.67793f-6 … -2.15211f-6 -5.14938f-6; 1.92656f-7 2.71131f-7 … 2.13291f-7 5.14152f-7; … ; 1.15218f-6 1.49191f-6 … 1.19111f-6 2.82344f-6; 1.18541f-6 1.60727f-6 … 1.29029f-6 3.11602f-6], Float32[4.22223f-13 7.17124f-13 … 4.63152f-13 2.65158f-12; 3.7116f-15 7.35113f-15 … 4.54925f-15 2.64348f-14; … ; 1.3275f-13 2.22577f-13 … 1.41873f-13 7.97171f-13; 1.40517f-13 2.58327f-13 … 1.66482f-13 9.70942f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-0.000622241, 5.53181f-5, 0.000229638, -6.1673f-5, 0.000169476, 0.000446214, 0.000706452, 0.000171382, -6.25591f-5, 0.000188743  …  -0.000187882, -0.000264875, -0.000697209, 6.18874f-5, -0.000171018, -0.000116905, 0.000218472, -3.38884f-5, 0.000335067, 0.000361899], Float32[3.87179f-8, 3.06005f-10, 5.27327f-9, 3.80351f-10, 2.87217f-9, 1.99104f-8, 4.99068f-8, 2.93713f-9, 3.91359f-10, 3.56233f-9  …  3.52991f-9, 7.0158f-9, 4.86095f-8, 3.83f-10, 2.92467f-9, 1.36667f-9, 4.77296f-9, 1.14841f-10, 1.12268f-8, 1.30969f-8], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ())),)), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[1.1709f-13 -2.17074f-13 … -2.41981f-13 5.02323f-14; 3.09099f-13 -5.49818f-13 … -6.30734f-13 1.31787f-13; … ; 3.22154f-13 -4.87063f-13 … -6.12884f-13 1.37014f-13; 8.0882f-13 -1.24605f-12 … -1.5365f-12 3.44892f-13], Float32[1.37099f-27 4.71203f-27 … 5.85542f-27 2.52325f-28; 9.55408f-27 3.02296f-26 … 3.9782f-26 1.73675f-27; … ; 1.03782f-26 2.37227f-26 … 3.75622f-26 1.87727f-27; 6.54181f-26 1.55262f-25 … 2.36081f-25 1.18949f-26], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-1.09619f-14 4.79692f-14 … 3.90143f-14 -1.25849f-14; 6.48557f-13 -1.15723f-12 … -1.32092f-12 2.76203f-13; … ; 5.57569f-13 -9.29016f-13 … -8.80362f-13 4.89454f-14; 1.67423f-14 -4.85356f-14 … -5.60699f-14 1.30596f-15], Float32[1.20162f-29 2.30101f-28 … 1.52209f-28 1.58379f-29; 4.20621f-26 1.33917f-25 … 1.74479f-25 7.62869f-27; … ; 3.10879f-26 8.63059f-26 … 7.75026f-26 2.39562f-28; 2.803f-29 2.35567f-28 … 3.14379f-28 1.70551f-31], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-1.28106f-7 1.8123f-7 … 2.93124f-7 -5.83281f-8; 6.10564f-7 -1.0218f-6 … -1.1785f-6 2.50165f-7; … ; -4.4433f-7 7.44488f-7 … 8.4444f-7 -1.599f-7; -8.91949f-7 1.44962f-6 … 1.68738f-6 -3.34733f-7], Float32[1.64109f-15 3.28437f-15 … 8.59208f-15 3.40212f-16; 3.72783f-14 1.04405f-13 … 1.38885f-13 6.25815f-15; … ; 1.97427f-14 5.54255f-14 … 7.1307f-14 2.55676f-15; 7.95562f-14 2.10138f-13 … 2.8472f-13 1.12045f-14], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-1.62062f-6 2.95075f-6 … 1.04192f-6 8.06289f-7; -7.27658f-7 1.38092f-6 … 6.99037f-7 3.99956f-7; … ; 2.10659f-6 -3.48698f-6 … -1.39887f-6 -1.00407f-6; 3.18165f-7 -8.91885f-7 … -4.84805f-7 -2.38268f-7], Float32[2.62637f-13 8.70679f-13 … 1.08559f-13 6.50093f-14; 5.29478f-14 1.9069f-13 … 4.88646f-14 1.59963f-14; … ; 4.43768f-13 1.21588f-12 … 1.9568f-13 1.00815f-13; 1.01227f-14 7.95448f-14 … 2.35032f-14 5.67708f-15], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; -2.31965f-7 -9.54908f-7 … -4.49589f-7 1.88726f-7; 1.75121f-7 9.91089f-7 … 6.65036f-7 -3.62272f-7], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 5.38068f-15 9.11836f-14 … 2.02127f-14 3.56172f-15; 3.06669f-15 9.82244f-14 … 4.42267f-14 1.31239f-14], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.000113338, -2.69748f-6, 0.000166572, 0.0, -0.000154423, -0.000502071, 0.0, -0.000215736  …  0.0, 0.0, 0.0, 0.000243336, -0.000469404, -0.000237652, 0.0, 0.0, -0.000149315, 0.000191518], Float32[0.0, 0.0, 1.28454f-9, 7.27628f-13, 2.77457f-9, 0.0, 2.38462f-9, 2.52072f-8, 0.0, 4.65413f-9  …  0.0, 0.0, 0.0, 5.92116f-9, 2.20337f-8, 5.64779f-9, 0.0, 0.0, 2.22947f-9, 3.66787f-9], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … -1.36481f-7 -2.00695f-6; 0.0 0.0 … -1.56637f-7 -1.74547f-6; … ; 0.0 0.0 … -1.7761f-8 -8.29992f-7; 0.0 0.0 … -1.05836f-8 -1.87974f-7], Float32[0.0 0.0 … 1.86269f-15 4.0278f-13; 0.0 0.0 … 2.45347f-15 3.04662f-13; … ; 0.0 0.0 … 3.15449f-17 6.88877f-14; 0.0 0.0 … 1.12012f-17 3.53336f-15], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-0.000683627, -0.000577462, -0.00069261, 0.000397629, -0.000352592, 3.2628f-5, 7.86947f-5, 1.37357f-5, 0.000261246, -0.000208792  …  0.00015684, 0.000276982, 0.000323947, -0.00104093, -0.000520947, 0.00123485, 0.000586675, -0.00035735, -0.000301812, -6.21945f-5], Float32[4.67339f-8, 3.33458f-8, 4.79702f-8, 1.58107f-8, 1.24319f-8, 1.06457f-10, 6.19277f-10, 1.88667f-11, 6.82484f-9, 4.35934f-9  …  2.45983f-9, 7.67179f-9, 1.04941f-8, 1.08351f-7, 2.71382f-8, 1.52484f-7, 3.44183f-8, 1.27698f-8, 9.1089f-9, 3.86811f-10], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ())),)), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[1.5308f-13 -8.75481f-14 … 3.33376f-14 -3.67564f-13; -6.8303f-14 3.9922f-14 … -1.30524f-14 1.69537f-13; … ; -4.35964f-15 7.25858f-15 … 1.86509f-16 1.78219f-14; 6.61177f-16 -5.61985f-15 … -9.77418f-16 -9.58068f-15], Float32[2.34331f-27 7.66456f-28 … 1.11138f-28 1.35101f-26; 4.66523f-28 1.59375f-28 … 1.70363f-29 2.87426f-27; … ; 1.90062f-30 5.26863f-30 … 3.47851f-33 3.17615f-29; 4.37149f-32 3.15823f-30 … 9.55334f-32 9.17882f-30], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-2.87766f-14 1.49383f-14 … -8.93341f-15 6.03485f-14; 2.01303f-13 -1.04457f-13 … 6.2953f-14 -4.17118f-13; … ; -5.7421f-15 -3.39976f-15 … -2.09927f-15 3.76f-16; -2.16422f-14 -1.01152f-14 … -8.9917f-15 7.59364f-16], Float32[8.28081f-29 2.23149f-29 … 7.98047f-30 3.6419f-28; 4.05225f-27 1.09112f-27 … 3.96302f-28 1.73985f-26; … ; 3.29712f-30 1.15582f-30 … 4.40687f-31 1.41374f-32; 4.68377f-29 1.02316f-29 … 8.08496f-30 5.76625f-32], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-2.98628f-7 1.96457f-7 … -8.19745f-8 6.69635f-7; 5.13266f-7 -2.70467f-7 … 1.60451f-7 -1.05651f-6; … ; -1.28877f-6 6.66985f-7 … -4.86275f-7 2.36508f-6; -1.4307f-6 8.81311f-7 … -4.28132f-7 3.02262f-6], Float32[8.91773f-15 3.85949f-15 … 6.71972f-16 4.48406f-14; 2.63438f-14 7.31513f-15 … 2.57443f-15 1.1162f-13; … ; 1.6609f-13 4.44863f-14 … 2.3646f-14 5.59352f-13; 2.04686f-13 7.76698f-14 … 1.83294f-14 9.13609f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-1.96254f-6 -1.87939f-6 … 2.27919f-6 -8.49468f-7; -4.45499f-6 -4.77064f-6 … 5.8699f-6 -2.54767f-6; … ; -4.26105f-7 -3.11804f-7 … 3.57741f-7 -6.09203f-8; 3.15805f-6 3.5671f-6 … -4.39515f-6 1.98458f-6], Float32[3.85153f-13 3.53204f-13 … 5.19462f-13 7.21586f-14; 1.98466f-12 2.27587f-12 … 3.44552f-12 6.49052f-13; … ; 1.81563f-14 9.72206f-15 … 1.27977f-14 3.71124f-16; 9.97315f-13 1.27241f-12 … 1.93171f-12 3.93852f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[5.10773f-9 3.95102f-6 … -1.33668f-6 2.38301f-6; 0.0 0.0 … 0.0 0.0; … ; 1.03734f-9 -4.22385f-8 … 1.17002f-8 -2.5601f-8; -5.74162f-8 -7.22821f-7 … 5.16317f-7 -6.70752f-7], Float32[2.60886f-18 1.56104f-12 … 1.78669f-13 5.67868f-13; 0.0 0.0 … 0.0 0.0; … ; 1.07605f-19 1.78407f-16 … 1.36893f-17 6.55403f-17; 3.29658f-16 5.22463f-14 … 2.6658f-14 4.49903f-14], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-0.000799955, 0.0, 0.000862772, 0.0, -0.000628598, 0.0, 0.0, 0.0, 0.0, -0.000865972  …  -0.000289518, -0.000201406, 0.000278765, 0.0, -0.000140399, 0.0, -0.000275755, 0.000293295, 9.05506f-6, 0.000161092], Float32[6.3992f-8, 0.0, 7.44365f-8, 0.0, 3.95131f-8, 0.0, 0.0, 0.0, 0.0, 7.49897f-8  …  8.38195f-9, 4.05638f-9, 7.77087f-9, 0.0, 1.97115f-9, 0.0, 7.60395f-9, 8.60207f-9, 8.19931f-12, 2.59503f-9], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[7.19242f-7 0.0 … 1.46315f-11 7.47498f-7; 1.2686f-6 0.0 … 7.90301f-12 1.37939f-6; … ; 6.63084f-7 0.0 … 4.75096f-12 7.57746f-7; -1.31364f-6 0.0 … 4.29087f-12 -1.47812f-6], Float32[5.17302f-14 0.0 … 2.14079f-23 5.58746f-14; 1.60932f-13 0.0 … 6.24567f-24 1.9027f-13; … ; 4.39674f-14 0.0 … 2.25713f-24 5.74171f-14; 1.72562f-13 0.0 … 1.84113f-24 2.18481f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.000406521, 0.000628218, 0.00144444, -0.000240913, -0.000427983, -0.00121615, -0.000507722, 0.000467283, -0.00145965, -0.000306283  …  0.000749912, -0.000232951, 0.000644978, 0.000777748, 0.00176201, -0.0015763, -0.000113564, 0.000978183, 0.000297818, -0.000591565], Float32[1.65257f-8, 3.94652f-8, 2.08639f-7, 5.80384f-9, 1.83167f-8, 1.47901f-7, 2.57779f-8, 2.18351f-8, 2.13054f-7, 9.3808f-9  …  5.6236f-8, 5.42657f-9, 4.15991f-8, 6.04884f-8, 3.10464f-7, 2.48468f-7, 1.28966f-9, 9.56828f-8, 8.86941f-9, 3.49945f-8], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ())),)), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-2.76641f-14 -6.36714f-14 … -8.96688f-14 -3.50892f-14; -8.64795f-15 -1.81483f-14 … -2.92004f-14 -1.0919f-14; … ; 1.7407f-15 3.99474f-15 … 5.12923f-15 2.33233f-15; 1.62173f-15 4.46748f-15 … 4.88832f-15 2.08368f-15], Float32[7.65293f-29 4.05399f-28 … 8.04038f-28 1.23123f-28; 7.4786f-30 3.29357f-29 … 8.52652f-29 1.19222f-29; … ; 3.03f-31 1.59577f-30 … 2.63086f-30 5.43968f-31; 2.62998f-31 1.99581f-30 … 2.38953f-30 4.34165f-31], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[8.54271f-15 1.91874f-14 … 3.10402f-14 1.04663f-14; 4.23224f-15 9.62661f-15 … 1.5092f-14 5.30759f-15; … ; 1.17964f-15 -7.44176f-16 … 9.34039f-15 4.85347f-16; 4.02068f-17 2.54007f-14 … -1.03655f-13 1.86683f-14], Float32[7.2977f-30 3.6815f-29 … 9.63482f-29 1.09541f-29; 1.79116f-30 9.26704f-30 … 2.27766f-29 2.81701f-30; … ; 1.39154f-31 5.53791f-32 … 8.72418f-30 2.35558f-32; 1.61657f-34 6.45188f-29 … 1.07443f-27 3.48502f-29], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-2.96289f-8 8.58508f-8 … -6.26159f-7 5.33869f-8; -3.19931f-6 -7.37156f-6 … -1.0337f-5 -4.12699f-6; … ; 1.19931f-6 2.81399f-6 … 3.77018f-6 1.543f-6; 1.46139f-6 3.26155f-6 … 5.06409f-6 1.81926f-6], Float32[8.7786f-17 7.37027f-16 … 3.9207f-14 2.85012f-16; 1.02355f-12 5.43392f-12 … 1.06853f-11 1.70318f-12; … ; 1.43832f-13 7.91841f-13 … 1.42141f-12 2.3808f-13; 2.13563f-13 1.06376f-12 … 2.56447f-12 3.30965f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[2.83063f-7 -1.5342f-7 … 6.75702f-7 -5.15972f-7; 1.92448f-6 -7.40836f-7 … 4.56612f-6 -3.02061f-6; … ; 5.81254f-7 -1.20743f-7 … 1.43664f-6 -8.55999f-7; -1.67623f-7 1.4355f-7 … -3.1795f-7 2.92165f-7], Float32[8.01237f-15 2.35373f-15 … 4.56567f-14 2.66224f-14; 3.70357f-13 5.48831f-14 … 2.08491f-12 9.12393f-13; … ; 3.37851f-14 1.45787f-15 … 2.06391f-13 7.32724f-14; 2.8097f-15 2.06062f-15 … 1.01091f-14 8.53591f-15], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-1.99159f-7 -4.84727f-7 … -4.95701f-7 -5.49486f-7; 0.0 0.0 … 0.0 0.0; … ; -1.30619f-6 -4.71422f-6 … -6.16669f-6 -5.50357f-6; 3.76901f-7 1.28247f-6 … 1.6177f-6 1.4823f-6], Float32[3.9664f-15 2.34958f-14 … 2.45716f-14 3.01931f-14; 0.0 0.0 … 0.0 0.0; … ; 1.70612f-13 2.22236f-12 … 3.80276f-12 3.02888f-12; 1.42053f-14 1.64472f-13 … 2.61693f-13 2.19719f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.000224313, 0.0, 0.0, 0.0, -0.000468287, 0.00167893, 0.00233493, -9.50693f-5, 0.0011317, 0.0  …  0.0, 0.00205225, 0.000717712, -0.000364723, 0.0, 0.0, 0.0, 0.0, 0.00195801, -0.000535957], Float32[5.03157f-9, 0.0, 0.0, 0.0, 2.1929f-8, 2.81875f-7, 5.45183f-7, 9.03805f-10, 1.28072f-7, 0.0  …  0.0, 4.21169f-7, 5.15103f-8, 1.33021f-8, 0.0, 0.0, 0.0, 0.0, 3.83377f-7, 2.87246f-8], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-1.23582f-6 0.0 … -3.74827f-7 -1.24156f-6; -3.63842f-6 0.0 … -1.20251f-6 -3.68756f-6; … ; -8.96747f-7 0.0 … -3.1666f-7 -9.35051f-7; 1.03367f-6 0.0 … 2.91327f-7 1.03446f-6], Float32[1.52722f-13 0.0 … 1.40494f-14 1.54145f-13; 1.32379f-12 0.0 … 1.44601f-13 1.35979f-12; … ; 8.04144f-14 0.0 … 1.00272f-14 8.74308f-14; 1.06845f-13 0.0 … 8.48703f-15 1.07009f-13], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-0.000773359, -0.0022245, 0.000798285, 0.000698303, -0.00364789, -0.00153132, -0.00268496, -2.37314f-5, -0.00183103, 0.00196001  …  0.000406504, 0.0020492, 0.00220228, -0.000324874, 0.00188294, 0.000865498, -0.0011559, 0.000311154, -0.000554234, 0.000660866], Float32[5.98076f-8, 4.94833f-7, 6.3725f-8, 4.87621f-8, 1.33069f-6, 2.3449f-7, 7.20892f-7, 5.63173f-11, 3.35264f-7, 3.84158f-7  …  1.65243f-8, 4.19916f-7, 4.84998f-7, 1.05542f-8, 3.5454f-7, 7.49076f-8, 1.3361f-7, 9.68157f-9, 3.07171f-8, 4.36739f-8], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ())),))], proj = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-2.25045f-7 4.62647f-7 … 6.28072f-8 1.53758f-8; -7.11927f-7 1.79362f-6 … -1.63994f-7 -1.23574f-6; … ; -7.88533f-7 1.99349f-6 … -1.47352f-7 -1.4001f-6; 1.86638f-6 -4.78276f-6 … 4.83925f-7 3.37834f-6], Float32[5.06446f-15 2.1404f-14 … 3.9447f-16 2.36413f-17; 5.06833f-14 3.21702f-13 … 2.68937f-15 1.52703f-13; … ; 6.21776f-14 3.97395f-13 … 2.17125f-15 1.96026f-13; 3.48332f-13 2.28745f-12 … 2.3418f-14 1.1413f-12], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[-0.000335827, -0.00169565, 0.00297636, 0.000234515, 0.00220194, 0.00278829, -0.0059942, 0.00396263, 0.00180509, 0.00141777  …  0.00122233, 0.00161248, -0.000539151, -0.000724104, -0.00677481, -0.00288134, -0.00442993, 0.00141357, -0.00189766, 0.00455024], Float32[1.12778f-8, 2.87519f-7, 8.8586f-7, 5.49966f-9, 4.84846f-7, 7.77445f-7, 3.59299f-6, 1.57023f-6, 3.25831f-7, 2.01005f-7  …  1.49408f-7, 2.60005f-7, 2.9068f-8, 5.24319f-8, 4.58974f-6, 8.302f-7, 1.9624f-6, 1.99817f-7, 3.60105f-7, 2.07044f-6], (0.81, 0.998001))\u001b[32m)\u001b[39m, σ = ())), ToyModel(Dense(22 => 128; bias=false), TransformerLayer[TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128))), TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128))), TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128))), TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128))), TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128))), TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128))), TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128))), TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128)))], Dense(128 => 22)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is an example of one training step. In the next cell we do this repeatedly to get our full training function, but by basically running this many times we train our model\n",
    "\n",
    "# We first choose one batch to train on\n",
    "batch = batches[1];\n",
    "# The input for our model will be all of the tokens in the sequence except of the final one\n",
    "input = batch[:,1:end-1,:];\n",
    "# What we then want the output to be is this sequence shifted one step to the right\n",
    "target = batch[:,2:end,:];\n",
    "input = Float32.(input)\n",
    "# We then compute the loss and gradients which we'll use to optimise our model\n",
    "loss, grads = Flux.withgradient(model) do m\n",
    "    # These are coming from the different between our model output and the actual targets (the sequence shifted one step to the right)\n",
    "    output = m(input)\n",
    "    Flux.logitcrossentropy(output, target)\n",
    "end\n",
    "# We then update our model and optimiser state\n",
    "opt_state, model = Optimisers.update(opt_state, model, grads[1])\n",
    "# By repeating this many times we can hopefully train a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_ar_model (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's our training function which basically just does what I described above but in a loop\n",
    "function train_ar_model(model, batches, opt_state)\n",
    "    losses = []\n",
    "    for batch in batches\n",
    "        input = batch[:,1:end-1,:]\n",
    "        input = Float32.(input)\n",
    "        target = batch[:,2:end,:]\n",
    "        loss, grads = Flux.withgradient(model) do m\n",
    "            output = m(input)\n",
    "            Flux.logitcrossentropy(output, target)\n",
    "        end\n",
    "        opt_state, model = Optimisers.update(opt_state, model, grads[1])\n",
    "        push!(losses, loss)\n",
    "        println(loss)\n",
    "    end\n",
    "    return model,losses\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0910225\n",
      "3.0673807\n",
      "3.7052412\n",
      "3.0309918\n",
      "3.0716307\n",
      "3.0701532\n",
      "3.0624862\n",
      "3.0437078\n",
      "2.9959075\n",
      "2.9104557\n",
      "3.1535554\n",
      "2.9014409\n",
      "2.970477\n",
      "3.0068064\n",
      "3.0104313\n",
      "3.0109904\n",
      "3.0109484\n",
      "2.9954617\n",
      "2.9760413\n",
      "2.9673629\n",
      "2.975095\n",
      "2.9653673\n",
      "2.9351118\n",
      "2.885145\n",
      "2.868559\n",
      "2.909339\n",
      "2.8654044\n",
      "2.867753\n",
      "2.8864205\n",
      "2.8867478\n",
      "2.8573341\n",
      "2.8764808\n",
      "2.8555698\n",
      "2.885088\n",
      "2.8575912\n",
      "2.8639982\n",
      "2.8558493\n",
      "2.864028\n",
      "2.891106\n",
      "2.8655367\n",
      "2.8755012\n",
      "2.8641007\n",
      "2.8954394\n",
      "2.8607183\n",
      "2.8648236\n",
      "2.8663602\n",
      "2.8702383\n",
      "2.8606453\n",
      "2.859753\n",
      "2.8631067\n",
      "2.8776898\n",
      "2.865365\n",
      "2.8269477\n",
      "2.8710337\n",
      "2.8829231\n",
      "2.8586326\n",
      "2.8955288\n",
      "2.8643296\n",
      "2.851835\n",
      "2.880405\n",
      "2.8795981\n",
      "2.8466334\n",
      "2.8627534\n",
      "2.8989425\n",
      "2.8984678\n",
      "2.873566\n",
      "2.8607984\n",
      "2.8625886\n",
      "2.8628364\n",
      "2.8566146\n",
      "2.8736265\n",
      "2.8887277\n",
      "2.842961\n",
      "2.8816404\n",
      "2.870431\n",
      "2.850738\n",
      "2.8743682\n",
      "2.8455973\n",
      "2.903488\n",
      "2.8778887\n",
      "2.856208\n",
      "2.8780775\n",
      "2.8584442\n",
      "2.8776886\n",
      "2.8801155\n",
      "2.8730252\n",
      "2.8825283\n",
      "2.8855872\n",
      "2.900773\n",
      "2.8656588\n",
      "2.8438947\n",
      "2.867957\n",
      "2.8726082\n",
      "2.8760486\n",
      "2.8315406\n",
      "2.8216126\n",
      "2.8794334\n",
      "2.856005\n",
      "2.8717942\n",
      "2.8395844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ToyModel(Dense(22 => 128; bias=false), TransformerLayer[TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128))), TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128))), TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128))), TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128))), TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128))), TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128))), TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128))), TransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(Dense(128 => 512, relu), Dense(512 => 128)))], Dense(128 => 22)), Any[3.0910225f0, 3.0673807f0, 3.7052412f0, 3.0309918f0, 3.0716307f0, 3.0701532f0, 3.0624862f0, 3.0437078f0, 2.9959075f0, 2.9104557f0  …  2.8438947f0, 2.867957f0, 2.8726082f0, 2.8760486f0, 2.8315406f0, 2.8216126f0, 2.8794334f0, 2.856005f0, 2.8717942f0, 2.8395844f0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I train a model on some antibody sequences and then print out the loss\n",
    "model, toy_loss = train_ar_model(model, batches[1:100], opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip020\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip020)\" d=\"M0 1600 L2400 1600 L2400 8.88178e-14 L0 8.88178e-14  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip021\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip020)\" d=\"M156.112 1486.45 L2352.76 1486.45 L2352.76 47.2441 L156.112 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip022\">\n",
       "    <rect x=\"156\" y=\"47\" width=\"2198\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip022)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"197.349,1486.45 197.349,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip022)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"720.658,1486.45 720.658,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip022)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1243.97,1486.45 1243.97,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip022)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1767.28,1486.45 1767.28,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip022)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2290.59,1486.45 2290.59,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip022)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"156.112,1478.92 2352.76,1478.92 \"/>\n",
       "<polyline clip-path=\"url(#clip022)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"156.112,1171.61 2352.76,1171.61 \"/>\n",
       "<polyline clip-path=\"url(#clip022)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"156.112,864.305 2352.76,864.305 \"/>\n",
       "<polyline clip-path=\"url(#clip022)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"156.112,556.995 2352.76,556.995 \"/>\n",
       "<polyline clip-path=\"url(#clip022)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"156.112,249.685 2352.76,249.685 \"/>\n",
       "<polyline clip-path=\"url(#clip020)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"156.112,1486.45 2352.76,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip020)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"197.349,1486.45 197.349,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip020)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"720.658,1486.45 720.658,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip020)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1243.97,1486.45 1243.97,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip020)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1767.28,1486.45 1767.28,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip020)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2290.59,1486.45 2290.59,1467.55 \"/>\n",
       "<path clip-path=\"url(#clip020)\" d=\"M197.349 1517.37 Q193.738 1517.37 191.909 1520.93 Q190.104 1524.47 190.104 1531.6 Q190.104 1538.71 191.909 1542.27 Q193.738 1545.82 197.349 1545.82 Q200.983 1545.82 202.789 1542.27 Q204.617 1538.71 204.617 1531.6 Q204.617 1524.47 202.789 1520.93 Q200.983 1517.37 197.349 1517.37 M197.349 1513.66 Q203.159 1513.66 206.215 1518.27 Q209.293 1522.85 209.293 1531.6 Q209.293 1540.33 206.215 1544.94 Q203.159 1549.52 197.349 1549.52 Q191.539 1549.52 188.46 1544.94 Q185.405 1540.33 185.405 1531.6 Q185.405 1522.85 188.46 1518.27 Q191.539 1513.66 197.349 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M699.929 1544.91 L716.249 1544.91 L716.249 1548.85 L694.304 1548.85 L694.304 1544.91 Q696.966 1542.16 701.55 1537.53 Q706.156 1532.88 707.337 1531.53 Q709.582 1529.01 710.462 1527.27 Q711.364 1525.51 711.364 1523.82 Q711.364 1521.07 709.42 1519.33 Q707.499 1517.6 704.397 1517.6 Q702.198 1517.6 699.744 1518.36 Q697.314 1519.13 694.536 1520.68 L694.536 1515.95 Q697.36 1514.82 699.814 1514.24 Q702.267 1513.66 704.304 1513.66 Q709.675 1513.66 712.869 1516.35 Q716.063 1519.03 716.063 1523.52 Q716.063 1525.65 715.253 1527.57 Q714.466 1529.47 712.36 1532.07 Q711.781 1532.74 708.679 1535.95 Q705.577 1539.15 699.929 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M726.11 1514.29 L744.466 1514.29 L744.466 1518.22 L730.392 1518.22 L730.392 1526.7 Q731.411 1526.35 732.429 1526.19 Q733.448 1526 734.466 1526 Q740.253 1526 743.633 1529.17 Q747.012 1532.34 747.012 1537.76 Q747.012 1543.34 743.54 1546.44 Q740.068 1549.52 733.749 1549.52 Q731.573 1549.52 729.304 1549.15 Q727.059 1548.78 724.651 1548.04 L724.651 1543.34 Q726.735 1544.47 728.957 1545.03 Q731.179 1545.58 733.656 1545.58 Q737.661 1545.58 739.999 1543.48 Q742.336 1541.37 742.336 1537.76 Q742.336 1534.15 739.999 1532.04 Q737.661 1529.94 733.656 1529.94 Q731.781 1529.94 729.906 1530.35 Q728.054 1530.77 726.11 1531.65 L726.11 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M1218.67 1514.29 L1237.02 1514.29 L1237.02 1518.22 L1222.95 1518.22 L1222.95 1526.7 Q1223.97 1526.35 1224.99 1526.19 Q1226 1526 1227.02 1526 Q1232.81 1526 1236.19 1529.17 Q1239.57 1532.34 1239.57 1537.76 Q1239.57 1543.34 1236.1 1546.44 Q1232.63 1549.52 1226.31 1549.52 Q1224.13 1549.52 1221.86 1549.15 Q1219.62 1548.78 1217.21 1548.04 L1217.21 1543.34 Q1219.29 1544.47 1221.51 1545.03 Q1223.74 1545.58 1226.21 1545.58 Q1230.22 1545.58 1232.56 1543.48 Q1234.89 1541.37 1234.89 1537.76 Q1234.89 1534.15 1232.56 1532.04 Q1230.22 1529.94 1226.21 1529.94 Q1224.34 1529.94 1222.46 1530.35 Q1220.61 1530.77 1218.67 1531.65 L1218.67 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M1258.78 1517.37 Q1255.17 1517.37 1253.34 1520.93 Q1251.54 1524.47 1251.54 1531.6 Q1251.54 1538.71 1253.34 1542.27 Q1255.17 1545.82 1258.78 1545.82 Q1262.42 1545.82 1264.22 1542.27 Q1266.05 1538.71 1266.05 1531.6 Q1266.05 1524.47 1264.22 1520.93 Q1262.42 1517.37 1258.78 1517.37 M1258.78 1513.66 Q1264.59 1513.66 1267.65 1518.27 Q1270.73 1522.85 1270.73 1531.6 Q1270.73 1540.33 1267.65 1544.94 Q1264.59 1549.52 1258.78 1549.52 Q1252.97 1549.52 1249.89 1544.94 Q1246.84 1540.33 1246.84 1531.6 Q1246.84 1522.85 1249.89 1518.27 Q1252.97 1513.66 1258.78 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M1741.13 1514.29 L1763.35 1514.29 L1763.35 1516.28 L1750.81 1548.85 L1745.92 1548.85 L1757.73 1518.22 L1741.13 1518.22 L1741.13 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M1772.52 1514.29 L1790.88 1514.29 L1790.88 1518.22 L1776.8 1518.22 L1776.8 1526.7 Q1777.82 1526.35 1778.84 1526.19 Q1779.86 1526 1780.88 1526 Q1786.66 1526 1790.04 1529.17 Q1793.42 1532.34 1793.42 1537.76 Q1793.42 1543.34 1789.95 1546.44 Q1786.48 1549.52 1780.16 1549.52 Q1777.98 1549.52 1775.71 1549.15 Q1773.47 1548.78 1771.06 1548.04 L1771.06 1543.34 Q1773.15 1544.47 1775.37 1545.03 Q1777.59 1545.58 1780.07 1545.58 Q1784.07 1545.58 1786.41 1543.48 Q1788.75 1541.37 1788.75 1537.76 Q1788.75 1534.15 1786.41 1532.04 Q1784.07 1529.94 1780.07 1529.94 Q1778.19 1529.94 1776.32 1530.35 Q1774.46 1530.77 1772.52 1531.65 L1772.52 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M2250.19 1544.91 L2257.83 1544.91 L2257.83 1518.55 L2249.52 1520.21 L2249.52 1515.95 L2257.79 1514.29 L2262.46 1514.29 L2262.46 1544.91 L2270.1 1544.91 L2270.1 1548.85 L2250.19 1548.85 L2250.19 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M2289.55 1517.37 Q2285.93 1517.37 2284.11 1520.93 Q2282.3 1524.47 2282.3 1531.6 Q2282.3 1538.71 2284.11 1542.27 Q2285.93 1545.82 2289.55 1545.82 Q2293.18 1545.82 2294.98 1542.27 Q2296.81 1538.71 2296.81 1531.6 Q2296.81 1524.47 2294.98 1520.93 Q2293.18 1517.37 2289.55 1517.37 M2289.55 1513.66 Q2295.36 1513.66 2298.41 1518.27 Q2301.49 1522.85 2301.49 1531.6 Q2301.49 1540.33 2298.41 1544.94 Q2295.36 1549.52 2289.55 1549.52 Q2283.73 1549.52 2280.66 1544.94 Q2277.6 1540.33 2277.6 1531.6 Q2277.6 1522.85 2280.66 1518.27 Q2283.73 1513.66 2289.55 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M2319.71 1517.37 Q2316.1 1517.37 2314.27 1520.93 Q2312.46 1524.47 2312.46 1531.6 Q2312.46 1538.71 2314.27 1542.27 Q2316.1 1545.82 2319.71 1545.82 Q2323.34 1545.82 2325.15 1542.27 Q2326.98 1538.71 2326.98 1531.6 Q2326.98 1524.47 2325.15 1520.93 Q2323.34 1517.37 2319.71 1517.37 M2319.71 1513.66 Q2325.52 1513.66 2328.57 1518.27 Q2331.65 1522.85 2331.65 1531.6 Q2331.65 1540.33 2328.57 1544.94 Q2325.52 1549.52 2319.71 1549.52 Q2313.9 1549.52 2310.82 1544.94 Q2307.76 1540.33 2307.76 1531.6 Q2307.76 1522.85 2310.82 1518.27 Q2313.9 1513.66 2319.71 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip020)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"156.112,1486.45 156.112,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip020)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"156.112,1478.92 175.01,1478.92 \"/>\n",
       "<polyline clip-path=\"url(#clip020)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"156.112,1171.61 175.01,1171.61 \"/>\n",
       "<polyline clip-path=\"url(#clip020)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"156.112,864.305 175.01,864.305 \"/>\n",
       "<polyline clip-path=\"url(#clip020)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"156.112,556.995 175.01,556.995 \"/>\n",
       "<polyline clip-path=\"url(#clip020)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"156.112,249.685 175.01,249.685 \"/>\n",
       "<path clip-path=\"url(#clip020)\" d=\"M57.0569 1492.27 L73.3763 1492.27 L73.3763 1496.2 L51.4319 1496.2 L51.4319 1492.27 Q54.094 1489.51 58.6773 1484.89 Q63.2837 1480.23 64.4643 1478.89 Q66.7096 1476.37 67.5893 1474.63 Q68.492 1472.87 68.492 1471.18 Q68.492 1468.43 66.5476 1466.69 Q64.6263 1464.95 61.5245 1464.95 Q59.3254 1464.95 56.8717 1465.72 Q54.4412 1466.48 51.6634 1468.03 L51.6634 1463.31 Q54.4875 1462.18 56.9412 1461.6 Q59.3949 1461.02 61.4319 1461.02 Q66.8022 1461.02 69.9967 1463.7 Q73.1911 1466.39 73.1911 1470.88 Q73.1911 1473.01 72.3809 1474.93 Q71.5939 1476.83 69.4874 1479.42 Q68.9087 1480.09 65.8069 1483.31 Q62.705 1486.51 57.0569 1492.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M83.191 1490.32 L88.0753 1490.32 L88.0753 1496.2 L83.191 1496.2 L83.191 1490.32 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M108.26 1479.79 Q104.927 1479.79 103.006 1481.57 Q101.108 1483.36 101.108 1486.48 Q101.108 1489.61 103.006 1491.39 Q104.927 1493.17 108.26 1493.17 Q111.594 1493.17 113.515 1491.39 Q115.436 1489.58 115.436 1486.48 Q115.436 1483.36 113.515 1481.57 Q111.617 1479.79 108.26 1479.79 M103.584 1477.8 Q100.575 1477.06 98.8854 1475 Q97.2187 1472.94 97.2187 1469.98 Q97.2187 1465.83 100.159 1463.43 Q103.121 1461.02 108.26 1461.02 Q113.422 1461.02 116.362 1463.43 Q119.302 1465.83 119.302 1469.98 Q119.302 1472.94 117.612 1475 Q115.945 1477.06 112.959 1477.8 Q116.339 1478.59 118.214 1480.88 Q120.112 1483.17 120.112 1486.48 Q120.112 1491.51 117.033 1494.19 Q113.978 1496.88 108.26 1496.88 Q102.543 1496.88 99.4641 1494.19 Q96.4085 1491.51 96.4085 1486.48 Q96.4085 1483.17 98.3067 1480.88 Q100.205 1478.59 103.584 1477.8 M101.871 1470.42 Q101.871 1473.1 103.538 1474.61 Q105.228 1476.11 108.26 1476.11 Q111.27 1476.11 112.959 1474.61 Q114.672 1473.1 114.672 1470.42 Q114.672 1467.73 112.959 1466.23 Q111.27 1464.72 108.26 1464.72 Q105.228 1464.72 103.538 1466.23 Q101.871 1467.73 101.871 1470.42 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M67.1032 1170.26 Q70.4596 1170.98 72.3346 1173.25 Q74.2327 1175.51 74.2327 1178.85 Q74.2327 1183.96 70.7142 1186.76 Q67.1957 1189.57 60.7143 1189.57 Q58.5384 1189.57 56.2236 1189.13 Q53.9319 1188.71 51.4782 1187.85 L51.4782 1183.34 Q53.4227 1184.47 55.7375 1185.05 Q58.0523 1185.63 60.5754 1185.63 Q64.9735 1185.63 67.2652 1183.89 Q69.58 1182.16 69.58 1178.85 Q69.58 1175.79 67.4272 1174.08 Q65.2976 1172.34 61.4782 1172.34 L57.4504 1172.34 L57.4504 1168.5 L61.6634 1168.5 Q65.1124 1168.5 66.9411 1167.14 Q68.7698 1165.75 68.7698 1163.15 Q68.7698 1160.49 66.8717 1159.08 Q64.9967 1157.64 61.4782 1157.64 Q59.5569 1157.64 57.3578 1158.06 Q55.1588 1158.48 52.5199 1159.36 L52.5199 1155.19 Q55.1819 1154.45 57.4967 1154.08 Q59.8347 1153.71 61.8948 1153.71 Q67.2189 1153.71 70.3207 1156.14 Q73.4226 1158.55 73.4226 1162.67 Q73.4226 1165.54 71.7791 1167.53 Q70.1355 1169.5 67.1032 1170.26 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M83.0984 1183.01 L87.9827 1183.01 L87.9827 1188.89 L83.0984 1188.89 L83.0984 1183.01 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M108.168 1157.41 Q104.557 1157.41 102.728 1160.98 Q100.922 1164.52 100.922 1171.65 Q100.922 1178.76 102.728 1182.32 Q104.557 1185.86 108.168 1185.86 Q111.802 1185.86 113.608 1182.32 Q115.436 1178.76 115.436 1171.65 Q115.436 1164.52 113.608 1160.98 Q111.802 1157.41 108.168 1157.41 M108.168 1153.71 Q113.978 1153.71 117.033 1158.32 Q120.112 1162.9 120.112 1171.65 Q120.112 1180.38 117.033 1184.98 Q113.978 1189.57 108.168 1189.57 Q102.358 1189.57 99.2789 1184.98 Q96.2234 1180.38 96.2234 1171.65 Q96.2234 1162.9 99.2789 1158.32 Q102.358 1153.71 108.168 1153.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M68.7004 862.95 Q72.0568 863.668 73.9318 865.937 Q75.83 868.205 75.83 871.538 Q75.83 876.654 72.3115 879.455 Q68.793 882.256 62.3115 882.256 Q60.1356 882.256 57.8208 881.816 Q55.5291 881.399 53.0754 880.543 L53.0754 876.029 Q55.0199 877.163 57.3347 877.742 Q59.6495 878.321 62.1726 878.321 Q66.5707 878.321 68.8624 876.585 Q71.1772 874.849 71.1772 871.538 Q71.1772 868.483 69.0244 866.77 Q66.8948 865.034 63.0754 865.034 L59.0476 865.034 L59.0476 861.191 L63.2606 861.191 Q66.7096 861.191 68.5383 859.825 Q70.367 858.437 70.367 855.844 Q70.367 853.182 68.4689 851.77 Q66.5939 850.335 63.0754 850.335 Q61.1541 850.335 58.955 850.751 Q56.756 851.168 54.1171 852.048 L54.1171 847.881 Q56.7791 847.14 59.0939 846.77 Q61.4319 846.4 63.4921 846.4 Q68.8161 846.4 71.9179 848.83 Q75.0198 851.238 75.0198 855.358 Q75.0198 858.228 73.3763 860.219 Q71.7328 862.187 68.7004 862.95 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M84.6956 875.705 L89.5799 875.705 L89.5799 881.585 L84.6956 881.585 L84.6956 875.705 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M103.793 877.649 L120.112 877.649 L120.112 881.585 L98.1678 881.585 L98.1678 877.649 Q100.83 874.895 105.413 870.265 Q110.02 865.612 111.2 864.27 Q113.445 861.747 114.325 860.011 Q115.228 858.251 115.228 856.562 Q115.228 853.807 113.283 852.071 Q111.362 850.335 108.26 850.335 Q106.061 850.335 103.608 851.099 Q101.177 851.863 98.3993 853.413 L98.3993 848.691 Q101.223 847.557 103.677 846.978 Q106.131 846.4 108.168 846.4 Q113.538 846.4 116.733 849.085 Q119.927 851.77 119.927 856.261 Q119.927 858.39 119.117 860.312 Q118.33 862.21 116.223 864.802 Q115.645 865.474 112.543 868.691 Q109.441 871.886 103.793 877.649 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M66.617 555.64 Q69.9735 556.358 71.8485 558.627 Q73.7466 560.895 73.7466 564.228 Q73.7466 569.344 70.2281 572.145 Q66.7096 574.946 60.2282 574.946 Q58.0523 574.946 55.7375 574.506 Q53.4458 574.089 50.9921 573.233 L50.9921 568.719 Q52.9366 569.853 55.2514 570.432 Q57.5662 571.011 60.0893 571.011 Q64.4874 571.011 66.7791 569.275 Q69.0939 567.539 69.0939 564.228 Q69.0939 561.173 66.9411 559.46 Q64.8115 557.724 60.9921 557.724 L56.9643 557.724 L56.9643 553.881 L61.1773 553.881 Q64.6263 553.881 66.455 552.515 Q68.2837 551.127 68.2837 548.534 Q68.2837 545.872 66.3856 544.46 Q64.5106 543.025 60.9921 543.025 Q59.0708 543.025 56.8717 543.441 Q54.6727 543.858 52.0338 544.738 L52.0338 540.571 Q54.6958 539.83 57.0106 539.46 Q59.3486 539.09 61.4087 539.09 Q66.7328 539.09 69.8346 541.52 Q72.9365 543.928 72.9365 548.048 Q72.9365 550.918 71.2929 552.909 Q69.6494 554.877 66.617 555.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M82.6123 568.395 L87.4966 568.395 L87.4966 574.275 L82.6123 574.275 L82.6123 568.395 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M110.529 543.789 L98.7234 562.238 L110.529 562.238 L110.529 543.789 M109.302 539.715 L115.182 539.715 L115.182 562.238 L120.112 562.238 L120.112 566.127 L115.182 566.127 L115.182 574.275 L110.529 574.275 L110.529 566.127 L94.9271 566.127 L94.9271 561.613 L109.302 539.715 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M66.9411 248.33 Q70.2976 249.048 72.1726 251.317 Q74.0707 253.585 74.0707 256.918 Q74.0707 262.034 70.5522 264.835 Q67.0337 267.636 60.5523 267.636 Q58.3763 267.636 56.0615 267.196 Q53.7699 266.779 51.3162 265.923 L51.3162 261.409 Q53.2606 262.543 55.5754 263.122 Q57.8902 263.701 60.4134 263.701 Q64.8115 263.701 67.1032 261.965 Q69.418 260.229 69.418 256.918 Q69.418 253.863 67.2652 252.15 Q65.1356 250.414 61.3161 250.414 L57.2884 250.414 L57.2884 246.571 L61.5013 246.571 Q64.9504 246.571 66.7791 245.205 Q68.6078 243.817 68.6078 241.224 Q68.6078 238.562 66.7096 237.15 Q64.8346 235.715 61.3161 235.715 Q59.3949 235.715 57.1958 236.131 Q54.9967 236.548 52.3579 237.428 L52.3579 233.261 Q55.0199 232.52 57.3347 232.15 Q59.6726 231.78 61.7328 231.78 Q67.0569 231.78 70.1587 234.21 Q73.2605 236.618 73.2605 240.738 Q73.2605 243.608 71.617 245.599 Q69.9735 247.567 66.9411 248.33 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M82.9364 261.085 L87.8206 261.085 L87.8206 266.965 L82.9364 266.965 L82.9364 261.085 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M108.584 247.821 Q105.436 247.821 103.584 249.974 Q101.756 252.127 101.756 255.877 Q101.756 259.604 103.584 261.779 Q105.436 263.932 108.584 263.932 Q111.733 263.932 113.561 261.779 Q115.413 259.604 115.413 255.877 Q115.413 252.127 113.561 249.974 Q111.733 247.821 108.584 247.821 M117.867 233.169 L117.867 237.428 Q116.108 236.594 114.302 236.155 Q112.52 235.715 110.76 235.715 Q106.131 235.715 103.677 238.84 Q101.246 241.965 100.899 248.284 Q102.265 246.27 104.325 245.205 Q106.385 244.118 108.862 244.118 Q114.07 244.118 117.08 247.289 Q120.112 250.437 120.112 255.877 Q120.112 261.201 116.964 264.418 Q113.816 267.636 108.584 267.636 Q102.589 267.636 99.4178 263.053 Q96.2465 258.446 96.2465 249.719 Q96.2465 241.525 100.135 236.664 Q104.024 231.78 110.575 231.78 Q112.334 231.78 114.117 232.127 Q115.922 232.474 117.867 233.169 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip022)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"218.281,1031.75 239.214,1068.08 260.146,87.9763 281.078,1123.99 302.011,1061.55 322.943,1063.82 343.876,1075.6 364.808,1104.46 385.74,1177.9 406.673,1309.2 427.605,935.669 448.537,1323.06 469.47,1216.98 490.402,1161.16 511.335,1155.59 532.267,1154.73 553.199,1154.79 574.132,1178.59 595.064,1208.43 615.996,1221.76 636.929,1209.88 657.861,1224.83 678.794,1271.32 699.726,1348.1 720.658,1373.58 741.591,1310.92 762.523,1378.43 783.456,1374.82 804.388,1346.14 825.32,1345.63 846.253,1390.83 867.185,1361.41 888.117,1393.54 909.05,1348.18 929.982,1390.43 950.915,1380.59 971.847,1393.11 992.779,1380.54 1013.71,1338.94 1034.64,1378.22 1055.58,1362.91 1076.51,1380.43 1097.44,1332.28 1118.37,1385.63 1139.31,1379.32 1160.24,1376.96 1181.17,1371 1202.1,1385.74 1223.04,1387.11 1243.97,1381.96 1264.9,1359.55 1285.83,1378.49 1306.76,1437.52 1327.7,1369.78 1348.63,1351.51 1369.56,1388.83 1390.49,1332.14 1411.43,1380.08 1432.36,1399.28 1453.29,1355.38 1474.22,1356.62 1495.16,1407.27 1516.09,1382.5 1537.02,1326.89 1557.95,1327.62 1578.89,1365.89 1599.82,1385.5 1620.75,1382.75 1641.68,1382.37 1662.62,1391.93 1683.55,1365.79 1704.48,1342.59 1725.41,1412.91 1746.34,1353.48 1767.28,1370.7 1788.21,1400.96 1809.14,1364.65 1830.07,1408.86 1851.01,1319.91 1871.94,1359.24 1892.87,1392.56 1913.8,1358.95 1934.74,1389.12 1955.67,1359.55 1976.6,1355.82 1997.53,1366.72 2018.47,1352.12 2039.4,1347.42 2060.33,1324.08 2081.26,1378.04 2102.2,1411.48 2123.13,1374.51 2144.06,1367.36 2164.99,1362.07 2185.92,1430.46 2206.86,1445.72 2227.79,1356.87 2248.72,1392.87 2269.65,1368.61 2290.59,1418.1 \"/>\n",
       "<path clip-path=\"url(#clip020)\" d=\"M2007.44 198.898 L2279.53 198.898 L2279.53 95.2176 L2007.44 95.2176  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip020)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2007.44,198.898 2279.53,198.898 2279.53,95.2176 2007.44,95.2176 2007.44,198.898 \"/>\n",
       "<polyline clip-path=\"url(#clip020)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2031.85,147.058 2178.29,147.058 \"/>\n",
       "<path clip-path=\"url(#clip020)\" d=\"M2216.54 166.745 Q2214.73 171.375 2213.02 172.787 Q2211.31 174.199 2208.44 174.199 L2205.03 174.199 L2205.03 170.634 L2207.53 170.634 Q2209.29 170.634 2210.27 169.8 Q2211.24 168.967 2212.42 165.865 L2213.18 163.921 L2202.7 138.412 L2207.21 138.412 L2215.31 158.689 L2223.41 138.412 L2227.93 138.412 L2216.54 166.745 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip020)\" d=\"M2235.22 160.402 L2242.86 160.402 L2242.86 134.037 L2234.55 135.703 L2234.55 131.444 L2242.81 129.778 L2247.49 129.778 L2247.49 160.402 L2255.13 160.402 L2255.13 164.338 L2235.22 164.338 L2235.22 160.402 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ],
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip070\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip070)\" d=\"M0 1600 L2400 1600 L2400 8.88178e-14 L0 8.88178e-14  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip071\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip070)\" d=\"M156.112 1486.45 L2352.76 1486.45 L2352.76 47.2441 L156.112 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip072\">\n",
       "    <rect x=\"156\" y=\"47\" width=\"2198\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"197.349,1486.45 197.349,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"720.658,1486.45 720.658,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1243.97,1486.45 1243.97,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1767.28,1486.45 1767.28,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2290.59,1486.45 2290.59,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"156.112,1478.92 2352.76,1478.92 \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"156.112,1171.61 2352.76,1171.61 \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"156.112,864.305 2352.76,864.305 \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"156.112,556.995 2352.76,556.995 \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"156.112,249.685 2352.76,249.685 \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"156.112,1486.45 2352.76,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"197.349,1486.45 197.349,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"720.658,1486.45 720.658,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1243.97,1486.45 1243.97,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1767.28,1486.45 1767.28,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2290.59,1486.45 2290.59,1467.55 \"/>\n",
       "<path clip-path=\"url(#clip070)\" d=\"M197.349 1517.37 Q193.738 1517.37 191.909 1520.93 Q190.104 1524.47 190.104 1531.6 Q190.104 1538.71 191.909 1542.27 Q193.738 1545.82 197.349 1545.82 Q200.983 1545.82 202.789 1542.27 Q204.617 1538.71 204.617 1531.6 Q204.617 1524.47 202.789 1520.93 Q200.983 1517.37 197.349 1517.37 M197.349 1513.66 Q203.159 1513.66 206.215 1518.27 Q209.293 1522.85 209.293 1531.6 Q209.293 1540.33 206.215 1544.94 Q203.159 1549.52 197.349 1549.52 Q191.539 1549.52 188.46 1544.94 Q185.405 1540.33 185.405 1531.6 Q185.405 1522.85 188.46 1518.27 Q191.539 1513.66 197.349 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M699.929 1544.91 L716.249 1544.91 L716.249 1548.85 L694.304 1548.85 L694.304 1544.91 Q696.966 1542.16 701.55 1537.53 Q706.156 1532.88 707.337 1531.53 Q709.582 1529.01 710.462 1527.27 Q711.364 1525.51 711.364 1523.82 Q711.364 1521.07 709.42 1519.33 Q707.499 1517.6 704.397 1517.6 Q702.198 1517.6 699.744 1518.36 Q697.314 1519.13 694.536 1520.68 L694.536 1515.95 Q697.36 1514.82 699.814 1514.24 Q702.267 1513.66 704.304 1513.66 Q709.675 1513.66 712.869 1516.35 Q716.063 1519.03 716.063 1523.52 Q716.063 1525.65 715.253 1527.57 Q714.466 1529.47 712.36 1532.07 Q711.781 1532.74 708.679 1535.95 Q705.577 1539.15 699.929 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M726.11 1514.29 L744.466 1514.29 L744.466 1518.22 L730.392 1518.22 L730.392 1526.7 Q731.411 1526.35 732.429 1526.19 Q733.448 1526 734.466 1526 Q740.253 1526 743.633 1529.17 Q747.012 1532.34 747.012 1537.76 Q747.012 1543.34 743.54 1546.44 Q740.068 1549.52 733.749 1549.52 Q731.573 1549.52 729.304 1549.15 Q727.059 1548.78 724.651 1548.04 L724.651 1543.34 Q726.735 1544.47 728.957 1545.03 Q731.179 1545.58 733.656 1545.58 Q737.661 1545.58 739.999 1543.48 Q742.336 1541.37 742.336 1537.76 Q742.336 1534.15 739.999 1532.04 Q737.661 1529.94 733.656 1529.94 Q731.781 1529.94 729.906 1530.35 Q728.054 1530.77 726.11 1531.65 L726.11 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1218.67 1514.29 L1237.02 1514.29 L1237.02 1518.22 L1222.95 1518.22 L1222.95 1526.7 Q1223.97 1526.35 1224.99 1526.19 Q1226 1526 1227.02 1526 Q1232.81 1526 1236.19 1529.17 Q1239.57 1532.34 1239.57 1537.76 Q1239.57 1543.34 1236.1 1546.44 Q1232.63 1549.52 1226.31 1549.52 Q1224.13 1549.52 1221.86 1549.15 Q1219.62 1548.78 1217.21 1548.04 L1217.21 1543.34 Q1219.29 1544.47 1221.51 1545.03 Q1223.74 1545.58 1226.21 1545.58 Q1230.22 1545.58 1232.56 1543.48 Q1234.89 1541.37 1234.89 1537.76 Q1234.89 1534.15 1232.56 1532.04 Q1230.22 1529.94 1226.21 1529.94 Q1224.34 1529.94 1222.46 1530.35 Q1220.61 1530.77 1218.67 1531.65 L1218.67 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1258.78 1517.37 Q1255.17 1517.37 1253.34 1520.93 Q1251.54 1524.47 1251.54 1531.6 Q1251.54 1538.71 1253.34 1542.27 Q1255.17 1545.82 1258.78 1545.82 Q1262.42 1545.82 1264.22 1542.27 Q1266.05 1538.71 1266.05 1531.6 Q1266.05 1524.47 1264.22 1520.93 Q1262.42 1517.37 1258.78 1517.37 M1258.78 1513.66 Q1264.59 1513.66 1267.65 1518.27 Q1270.73 1522.85 1270.73 1531.6 Q1270.73 1540.33 1267.65 1544.94 Q1264.59 1549.52 1258.78 1549.52 Q1252.97 1549.52 1249.89 1544.94 Q1246.84 1540.33 1246.84 1531.6 Q1246.84 1522.85 1249.89 1518.27 Q1252.97 1513.66 1258.78 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1741.13 1514.29 L1763.35 1514.29 L1763.35 1516.28 L1750.81 1548.85 L1745.92 1548.85 L1757.73 1518.22 L1741.13 1518.22 L1741.13 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1772.52 1514.29 L1790.88 1514.29 L1790.88 1518.22 L1776.8 1518.22 L1776.8 1526.7 Q1777.82 1526.35 1778.84 1526.19 Q1779.86 1526 1780.88 1526 Q1786.66 1526 1790.04 1529.17 Q1793.42 1532.34 1793.42 1537.76 Q1793.42 1543.34 1789.95 1546.44 Q1786.48 1549.52 1780.16 1549.52 Q1777.98 1549.52 1775.71 1549.15 Q1773.47 1548.78 1771.06 1548.04 L1771.06 1543.34 Q1773.15 1544.47 1775.37 1545.03 Q1777.59 1545.58 1780.07 1545.58 Q1784.07 1545.58 1786.41 1543.48 Q1788.75 1541.37 1788.75 1537.76 Q1788.75 1534.15 1786.41 1532.04 Q1784.07 1529.94 1780.07 1529.94 Q1778.19 1529.94 1776.32 1530.35 Q1774.46 1530.77 1772.52 1531.65 L1772.52 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M2250.19 1544.91 L2257.83 1544.91 L2257.83 1518.55 L2249.52 1520.21 L2249.52 1515.95 L2257.79 1514.29 L2262.46 1514.29 L2262.46 1544.91 L2270.1 1544.91 L2270.1 1548.85 L2250.19 1548.85 L2250.19 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M2289.55 1517.37 Q2285.93 1517.37 2284.11 1520.93 Q2282.3 1524.47 2282.3 1531.6 Q2282.3 1538.71 2284.11 1542.27 Q2285.93 1545.82 2289.55 1545.82 Q2293.18 1545.82 2294.98 1542.27 Q2296.81 1538.71 2296.81 1531.6 Q2296.81 1524.47 2294.98 1520.93 Q2293.18 1517.37 2289.55 1517.37 M2289.55 1513.66 Q2295.36 1513.66 2298.41 1518.27 Q2301.49 1522.85 2301.49 1531.6 Q2301.49 1540.33 2298.41 1544.94 Q2295.36 1549.52 2289.55 1549.52 Q2283.73 1549.52 2280.66 1544.94 Q2277.6 1540.33 2277.6 1531.6 Q2277.6 1522.85 2280.66 1518.27 Q2283.73 1513.66 2289.55 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M2319.71 1517.37 Q2316.1 1517.37 2314.27 1520.93 Q2312.46 1524.47 2312.46 1531.6 Q2312.46 1538.71 2314.27 1542.27 Q2316.1 1545.82 2319.71 1545.82 Q2323.34 1545.82 2325.15 1542.27 Q2326.98 1538.71 2326.98 1531.6 Q2326.98 1524.47 2325.15 1520.93 Q2323.34 1517.37 2319.71 1517.37 M2319.71 1513.66 Q2325.52 1513.66 2328.57 1518.27 Q2331.65 1522.85 2331.65 1531.6 Q2331.65 1540.33 2328.57 1544.94 Q2325.52 1549.52 2319.71 1549.52 Q2313.9 1549.52 2310.82 1544.94 Q2307.76 1540.33 2307.76 1531.6 Q2307.76 1522.85 2310.82 1518.27 Q2313.9 1513.66 2319.71 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"156.112,1486.45 156.112,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"156.112,1478.92 175.01,1478.92 \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"156.112,1171.61 175.01,1171.61 \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"156.112,864.305 175.01,864.305 \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"156.112,556.995 175.01,556.995 \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"156.112,249.685 175.01,249.685 \"/>\n",
       "<path clip-path=\"url(#clip070)\" d=\"M57.0569 1492.27 L73.3763 1492.27 L73.3763 1496.2 L51.4319 1496.2 L51.4319 1492.27 Q54.094 1489.51 58.6773 1484.89 Q63.2837 1480.23 64.4643 1478.89 Q66.7096 1476.37 67.5893 1474.63 Q68.492 1472.87 68.492 1471.18 Q68.492 1468.43 66.5476 1466.69 Q64.6263 1464.95 61.5245 1464.95 Q59.3254 1464.95 56.8717 1465.72 Q54.4412 1466.48 51.6634 1468.03 L51.6634 1463.31 Q54.4875 1462.18 56.9412 1461.6 Q59.3949 1461.02 61.4319 1461.02 Q66.8022 1461.02 69.9967 1463.7 Q73.1911 1466.39 73.1911 1470.88 Q73.1911 1473.01 72.3809 1474.93 Q71.5939 1476.83 69.4874 1479.42 Q68.9087 1480.09 65.8069 1483.31 Q62.705 1486.51 57.0569 1492.27 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M83.191 1490.32 L88.0753 1490.32 L88.0753 1496.2 L83.191 1496.2 L83.191 1490.32 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M108.26 1479.79 Q104.927 1479.79 103.006 1481.57 Q101.108 1483.36 101.108 1486.48 Q101.108 1489.61 103.006 1491.39 Q104.927 1493.17 108.26 1493.17 Q111.594 1493.17 113.515 1491.39 Q115.436 1489.58 115.436 1486.48 Q115.436 1483.36 113.515 1481.57 Q111.617 1479.79 108.26 1479.79 M103.584 1477.8 Q100.575 1477.06 98.8854 1475 Q97.2187 1472.94 97.2187 1469.98 Q97.2187 1465.83 100.159 1463.43 Q103.121 1461.02 108.26 1461.02 Q113.422 1461.02 116.362 1463.43 Q119.302 1465.83 119.302 1469.98 Q119.302 1472.94 117.612 1475 Q115.945 1477.06 112.959 1477.8 Q116.339 1478.59 118.214 1480.88 Q120.112 1483.17 120.112 1486.48 Q120.112 1491.51 117.033 1494.19 Q113.978 1496.88 108.26 1496.88 Q102.543 1496.88 99.4641 1494.19 Q96.4085 1491.51 96.4085 1486.48 Q96.4085 1483.17 98.3067 1480.88 Q100.205 1478.59 103.584 1477.8 M101.871 1470.42 Q101.871 1473.1 103.538 1474.61 Q105.228 1476.11 108.26 1476.11 Q111.27 1476.11 112.959 1474.61 Q114.672 1473.1 114.672 1470.42 Q114.672 1467.73 112.959 1466.23 Q111.27 1464.72 108.26 1464.72 Q105.228 1464.72 103.538 1466.23 Q101.871 1467.73 101.871 1470.42 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M67.1032 1170.26 Q70.4596 1170.98 72.3346 1173.25 Q74.2327 1175.51 74.2327 1178.85 Q74.2327 1183.96 70.7142 1186.76 Q67.1957 1189.57 60.7143 1189.57 Q58.5384 1189.57 56.2236 1189.13 Q53.9319 1188.71 51.4782 1187.85 L51.4782 1183.34 Q53.4227 1184.47 55.7375 1185.05 Q58.0523 1185.63 60.5754 1185.63 Q64.9735 1185.63 67.2652 1183.89 Q69.58 1182.16 69.58 1178.85 Q69.58 1175.79 67.4272 1174.08 Q65.2976 1172.34 61.4782 1172.34 L57.4504 1172.34 L57.4504 1168.5 L61.6634 1168.5 Q65.1124 1168.5 66.9411 1167.14 Q68.7698 1165.75 68.7698 1163.15 Q68.7698 1160.49 66.8717 1159.08 Q64.9967 1157.64 61.4782 1157.64 Q59.5569 1157.64 57.3578 1158.06 Q55.1588 1158.48 52.5199 1159.36 L52.5199 1155.19 Q55.1819 1154.45 57.4967 1154.08 Q59.8347 1153.71 61.8948 1153.71 Q67.2189 1153.71 70.3207 1156.14 Q73.4226 1158.55 73.4226 1162.67 Q73.4226 1165.54 71.7791 1167.53 Q70.1355 1169.5 67.1032 1170.26 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M83.0984 1183.01 L87.9827 1183.01 L87.9827 1188.89 L83.0984 1188.89 L83.0984 1183.01 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M108.168 1157.41 Q104.557 1157.41 102.728 1160.98 Q100.922 1164.52 100.922 1171.65 Q100.922 1178.76 102.728 1182.32 Q104.557 1185.86 108.168 1185.86 Q111.802 1185.86 113.608 1182.32 Q115.436 1178.76 115.436 1171.65 Q115.436 1164.52 113.608 1160.98 Q111.802 1157.41 108.168 1157.41 M108.168 1153.71 Q113.978 1153.71 117.033 1158.32 Q120.112 1162.9 120.112 1171.65 Q120.112 1180.38 117.033 1184.98 Q113.978 1189.57 108.168 1189.57 Q102.358 1189.57 99.2789 1184.98 Q96.2234 1180.38 96.2234 1171.65 Q96.2234 1162.9 99.2789 1158.32 Q102.358 1153.71 108.168 1153.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M68.7004 862.95 Q72.0568 863.668 73.9318 865.937 Q75.83 868.205 75.83 871.538 Q75.83 876.654 72.3115 879.455 Q68.793 882.256 62.3115 882.256 Q60.1356 882.256 57.8208 881.816 Q55.5291 881.399 53.0754 880.543 L53.0754 876.029 Q55.0199 877.163 57.3347 877.742 Q59.6495 878.321 62.1726 878.321 Q66.5707 878.321 68.8624 876.585 Q71.1772 874.849 71.1772 871.538 Q71.1772 868.483 69.0244 866.77 Q66.8948 865.034 63.0754 865.034 L59.0476 865.034 L59.0476 861.191 L63.2606 861.191 Q66.7096 861.191 68.5383 859.825 Q70.367 858.437 70.367 855.844 Q70.367 853.182 68.4689 851.77 Q66.5939 850.335 63.0754 850.335 Q61.1541 850.335 58.955 850.751 Q56.756 851.168 54.1171 852.048 L54.1171 847.881 Q56.7791 847.14 59.0939 846.77 Q61.4319 846.4 63.4921 846.4 Q68.8161 846.4 71.9179 848.83 Q75.0198 851.238 75.0198 855.358 Q75.0198 858.228 73.3763 860.219 Q71.7328 862.187 68.7004 862.95 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M84.6956 875.705 L89.5799 875.705 L89.5799 881.585 L84.6956 881.585 L84.6956 875.705 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M103.793 877.649 L120.112 877.649 L120.112 881.585 L98.1678 881.585 L98.1678 877.649 Q100.83 874.895 105.413 870.265 Q110.02 865.612 111.2 864.27 Q113.445 861.747 114.325 860.011 Q115.228 858.251 115.228 856.562 Q115.228 853.807 113.283 852.071 Q111.362 850.335 108.26 850.335 Q106.061 850.335 103.608 851.099 Q101.177 851.863 98.3993 853.413 L98.3993 848.691 Q101.223 847.557 103.677 846.978 Q106.131 846.4 108.168 846.4 Q113.538 846.4 116.733 849.085 Q119.927 851.77 119.927 856.261 Q119.927 858.39 119.117 860.312 Q118.33 862.21 116.223 864.802 Q115.645 865.474 112.543 868.691 Q109.441 871.886 103.793 877.649 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M66.617 555.64 Q69.9735 556.358 71.8485 558.627 Q73.7466 560.895 73.7466 564.228 Q73.7466 569.344 70.2281 572.145 Q66.7096 574.946 60.2282 574.946 Q58.0523 574.946 55.7375 574.506 Q53.4458 574.089 50.9921 573.233 L50.9921 568.719 Q52.9366 569.853 55.2514 570.432 Q57.5662 571.011 60.0893 571.011 Q64.4874 571.011 66.7791 569.275 Q69.0939 567.539 69.0939 564.228 Q69.0939 561.173 66.9411 559.46 Q64.8115 557.724 60.9921 557.724 L56.9643 557.724 L56.9643 553.881 L61.1773 553.881 Q64.6263 553.881 66.455 552.515 Q68.2837 551.127 68.2837 548.534 Q68.2837 545.872 66.3856 544.46 Q64.5106 543.025 60.9921 543.025 Q59.0708 543.025 56.8717 543.441 Q54.6727 543.858 52.0338 544.738 L52.0338 540.571 Q54.6958 539.83 57.0106 539.46 Q59.3486 539.09 61.4087 539.09 Q66.7328 539.09 69.8346 541.52 Q72.9365 543.928 72.9365 548.048 Q72.9365 550.918 71.2929 552.909 Q69.6494 554.877 66.617 555.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M82.6123 568.395 L87.4966 568.395 L87.4966 574.275 L82.6123 574.275 L82.6123 568.395 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M110.529 543.789 L98.7234 562.238 L110.529 562.238 L110.529 543.789 M109.302 539.715 L115.182 539.715 L115.182 562.238 L120.112 562.238 L120.112 566.127 L115.182 566.127 L115.182 574.275 L110.529 574.275 L110.529 566.127 L94.9271 566.127 L94.9271 561.613 L109.302 539.715 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M66.9411 248.33 Q70.2976 249.048 72.1726 251.317 Q74.0707 253.585 74.0707 256.918 Q74.0707 262.034 70.5522 264.835 Q67.0337 267.636 60.5523 267.636 Q58.3763 267.636 56.0615 267.196 Q53.7699 266.779 51.3162 265.923 L51.3162 261.409 Q53.2606 262.543 55.5754 263.122 Q57.8902 263.701 60.4134 263.701 Q64.8115 263.701 67.1032 261.965 Q69.418 260.229 69.418 256.918 Q69.418 253.863 67.2652 252.15 Q65.1356 250.414 61.3161 250.414 L57.2884 250.414 L57.2884 246.571 L61.5013 246.571 Q64.9504 246.571 66.7791 245.205 Q68.6078 243.817 68.6078 241.224 Q68.6078 238.562 66.7096 237.15 Q64.8346 235.715 61.3161 235.715 Q59.3949 235.715 57.1958 236.131 Q54.9967 236.548 52.3579 237.428 L52.3579 233.261 Q55.0199 232.52 57.3347 232.15 Q59.6726 231.78 61.7328 231.78 Q67.0569 231.78 70.1587 234.21 Q73.2605 236.618 73.2605 240.738 Q73.2605 243.608 71.617 245.599 Q69.9735 247.567 66.9411 248.33 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M82.9364 261.085 L87.8206 261.085 L87.8206 266.965 L82.9364 266.965 L82.9364 261.085 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M108.584 247.821 Q105.436 247.821 103.584 249.974 Q101.756 252.127 101.756 255.877 Q101.756 259.604 103.584 261.779 Q105.436 263.932 108.584 263.932 Q111.733 263.932 113.561 261.779 Q115.413 259.604 115.413 255.877 Q115.413 252.127 113.561 249.974 Q111.733 247.821 108.584 247.821 M117.867 233.169 L117.867 237.428 Q116.108 236.594 114.302 236.155 Q112.52 235.715 110.76 235.715 Q106.131 235.715 103.677 238.84 Q101.246 241.965 100.899 248.284 Q102.265 246.27 104.325 245.205 Q106.385 244.118 108.862 244.118 Q114.07 244.118 117.08 247.289 Q120.112 250.437 120.112 255.877 Q120.112 261.201 116.964 264.418 Q113.816 267.636 108.584 267.636 Q102.589 267.636 99.4178 263.053 Q96.2465 258.446 96.2465 249.719 Q96.2465 241.525 100.135 236.664 Q104.024 231.78 110.575 231.78 Q112.334 231.78 114.117 232.127 Q115.922 232.474 117.867 233.169 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip072)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"218.281,1031.75 239.214,1068.08 260.146,87.9763 281.078,1123.99 302.011,1061.55 322.943,1063.82 343.876,1075.6 364.808,1104.46 385.74,1177.9 406.673,1309.2 427.605,935.669 448.537,1323.06 469.47,1216.98 490.402,1161.16 511.335,1155.59 532.267,1154.73 553.199,1154.79 574.132,1178.59 595.064,1208.43 615.996,1221.76 636.929,1209.88 657.861,1224.83 678.794,1271.32 699.726,1348.1 720.658,1373.58 741.591,1310.92 762.523,1378.43 783.456,1374.82 804.388,1346.14 825.32,1345.63 846.253,1390.83 867.185,1361.41 888.117,1393.54 909.05,1348.18 929.982,1390.43 950.915,1380.59 971.847,1393.11 992.779,1380.54 1013.71,1338.94 1034.64,1378.22 1055.58,1362.91 1076.51,1380.43 1097.44,1332.28 1118.37,1385.63 1139.31,1379.32 1160.24,1376.96 1181.17,1371 1202.1,1385.74 1223.04,1387.11 1243.97,1381.96 1264.9,1359.55 1285.83,1378.49 1306.76,1437.52 1327.7,1369.78 1348.63,1351.51 1369.56,1388.83 1390.49,1332.14 1411.43,1380.08 1432.36,1399.28 1453.29,1355.38 1474.22,1356.62 1495.16,1407.27 1516.09,1382.5 1537.02,1326.89 1557.95,1327.62 1578.89,1365.89 1599.82,1385.5 1620.75,1382.75 1641.68,1382.37 1662.62,1391.93 1683.55,1365.79 1704.48,1342.59 1725.41,1412.91 1746.34,1353.48 1767.28,1370.7 1788.21,1400.96 1809.14,1364.65 1830.07,1408.86 1851.01,1319.91 1871.94,1359.24 1892.87,1392.56 1913.8,1358.95 1934.74,1389.12 1955.67,1359.55 1976.6,1355.82 1997.53,1366.72 2018.47,1352.12 2039.4,1347.42 2060.33,1324.08 2081.26,1378.04 2102.2,1411.48 2123.13,1374.51 2144.06,1367.36 2164.99,1362.07 2185.92,1430.46 2206.86,1445.72 2227.79,1356.87 2248.72,1392.87 2269.65,1368.61 2290.59,1418.1 \"/>\n",
       "<path clip-path=\"url(#clip070)\" d=\"M2007.44 198.898 L2279.53 198.898 L2279.53 95.2176 L2007.44 95.2176  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2007.44,198.898 2279.53,198.898 2279.53,95.2176 2007.44,95.2176 2007.44,198.898 \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2031.85,147.058 2178.29,147.058 \"/>\n",
       "<path clip-path=\"url(#clip070)\" d=\"M2216.54 166.745 Q2214.73 171.375 2213.02 172.787 Q2211.31 174.199 2208.44 174.199 L2205.03 174.199 L2205.03 170.634 L2207.53 170.634 Q2209.29 170.634 2210.27 169.8 Q2211.24 168.967 2212.42 165.865 L2213.18 163.921 L2202.7 138.412 L2207.21 138.412 L2215.31 158.689 L2223.41 138.412 L2227.93 138.412 L2216.54 166.745 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M2235.22 160.402 L2242.86 160.402 L2242.86 134.037 L2234.55 135.703 L2234.55 131.444 L2242.81 129.778 L2247.49 129.778 L2247.49 160.402 L2255.13 160.402 L2255.13 164.338 L2235.22 164.338 L2235.22 160.402 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A plot of the loss\n",
    "using Plots\n",
    "plot(toy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is basically the same as the one above but with a few tricks, I'm only going to comment on these\n",
    "using Flux\n",
    "using Zygote: checkpointed\n",
    "using Statistics\n",
    "\n",
    "# For one I'm using a SwiGLU which is an activation function which one can use instead of ReLU which is the classical example\n",
    "# It gives better gradient flows in deep networks. Read more here: https://paperswithcode.com/method/swiglu\n",
    "struct SwiGLU\n",
    "    W1::Dense\n",
    "    W2::Dense\n",
    "end\n",
    "\n",
    "Flux.@layer SwiGLU\n",
    "\n",
    "function SwiGLU(in_dim::Int, hidden_dim::Int)\n",
    "    SwiGLU(\n",
    "        Dense(in_dim, hidden_dim),\n",
    "        Dense(in_dim, hidden_dim)\n",
    "    )\n",
    "end\n",
    "\n",
    "function (sg::SwiGLU)(x)\n",
    "    return sg.W1(x) .* Flux.sigmoid.(sg.W2(x))\n",
    "end\n",
    "\n",
    "struct ARTransformerLayer\n",
    "    attention::Attention\n",
    "    ff::Chain\n",
    "    # Here we also use normalisation to help stabilise the training: https://arxiv.org/abs/1910.07467\n",
    "    # https://arxiv.org/pdf/2002.04745\n",
    "    norm1::RMSNorm\n",
    "    norm2::RMSNorm\n",
    "    # I then also have dropout which is a trick which is used to prevent overfitting: https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9/\n",
    "    dropout::Dropout\n",
    "end\n",
    "\n",
    "Flux.@layer ARTransformerLayer\n",
    "\n",
    "function ARTransformerLayer(dim::Int, n_head::Int, dropout_rate::Float32)\n",
    "    return ARTransformerLayer(\n",
    "        Attention(dim, n_head),\n",
    "        Chain(\n",
    "            SwiGLU(dim, dim * 4),\n",
    "            Dense(dim * 4, dim)\n",
    "        ),\n",
    "        RMSNorm(dim),\n",
    "        RMSNorm(dim),\n",
    "        Dropout(dropout_rate)\n",
    "    )\n",
    "end\n",
    "\n",
    "function (layer::ARTransformerLayer)(x::AbstractArray)\n",
    "    seqlen = size(x, 2)\n",
    "    h = zeros(Float32, layer.attention.dim, seqlen, 1)\n",
    "    mask = causal_mask(h)\n",
    "    rope = RoPE(layer.attention.head_dim, seqlen)\n",
    "\n",
    "    attn_out = layer.attention(layer.norm1(x), 1, rope, mask)\n",
    "    x = x + Flux.dropout(attn_out, layer.dropout.p)\n",
    "\n",
    "    ff_out = layer.ff(layer.norm2(x))\n",
    "    x = x + Flux.dropout(ff_out, layer.dropout.p)\n",
    "    \n",
    "    return x\n",
    "end\n",
    "\n",
    "struct ImprovedModel\n",
    "    vocab_embed::Dense\n",
    "    layers::Vector{ARTransformerLayer}\n",
    "    final::Chain\n",
    "end\n",
    "\n",
    "Flux.@layer ImprovedModel\n",
    "\n",
    "function ImprovedModel(vocab_size::Int, dim::Int, n_layers::Int, n_heads::Int, dropout_rate::Float32)\n",
    "    vocab_embed = Dense(vocab_size, dim, bias=false)\n",
    "    layers = [ARTransformerLayer(dim, n_heads, dropout_rate) for _ in 1:n_layers]\n",
    "    final = Chain(\n",
    "        RMSNorm(dim),\n",
    "        Dense(dim, vocab_size)\n",
    "    )\n",
    "    return ImprovedModel(vocab_embed, layers, final)\n",
    "end\n",
    "\n",
    "function (model::ImprovedModel)(x::AbstractArray)\n",
    "    x = model.vocab_embed(x)\n",
    "    for layer in model.layers\n",
    "        # Here I also use gradient checkpointing which is a trick to allow for bigger models: https://paperswithcode.com/method/gradient-checkpointing\n",
    "        x = checkpointed(layer, x)\n",
    "    end\n",
    "    return model.final(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(vocab_embed = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), layers = @NamedTuple{attention::@NamedTuple{wq::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, wk::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, wv::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, wo::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Tuple{}, σ::Tuple{}}, dim::Tuple{}, n_heads::Tuple{}, n_kv_heads::Tuple{}, head_dim::Tuple{}}, ff::@NamedTuple{layers::Tuple{@NamedTuple{W1::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, σ::Tuple{}}, W2::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, σ::Tuple{}}}, @NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, σ::Tuple{}}}}, norm1::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, eps::Tuple{}}, norm2::@NamedTuple{weight::Optimisers.Leaf{Adam{Float32, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}, eps::Tuple{}}, dropout::@NamedTuple{p::Tuple{}, dims::Tuple{}, active::Tuple{}, rng::Tuple{}}}[(attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((W1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), W2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),), norm1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), norm2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), dropout = (p = (), dims = (), active = (), rng = ())), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((W1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), W2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),), norm1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), norm2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), dropout = (p = (), dims = (), active = (), rng = ())), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((W1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), W2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),), norm1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), norm2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), dropout = (p = (), dims = (), active = (), rng = ())), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((W1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), W2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),), norm1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), norm2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), dropout = (p = (), dims = (), active = (), rng = ())), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((W1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), W2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),), norm1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), norm2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), dropout = (p = (), dims = (), active = (), rng = ())), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((W1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), W2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),), norm1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), norm2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), dropout = (p = (), dims = (), active = (), rng = ())), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((W1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), W2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),), norm1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), norm2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), dropout = (p = (), dims = (), active = (), rng = ())), (attention = (wq = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wk = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wv = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), wo = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = (), σ = ()), dim = (), n_heads = (), n_kv_heads = (), head_dim = ()), ff = (layers = ((W1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), W2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),), norm1 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), norm2 = (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), dropout = (p = (), dims = (), active = (), rng = ()))], final = (layers = ((weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, eps = ()), (weight = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_rate = 0.1f0\n",
    "layers = 8\n",
    "dim = 128\n",
    "n_heads = 2\n",
    " \n",
    "model = ImprovedModel(vocab_size, dim, layers, n_heads, dropout_rate)\n",
    "base_lr = 0.001f0\n",
    "rule = Optimisers.Adam(base_lr) \n",
    "opt_state = Optimisers.setup(rule, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9736392\n",
      "3.827649\n",
      "3.7222044\n",
      "3.2871213\n",
      "3.0097704\n",
      "2.8940656\n",
      "2.8884637\n",
      "2.8412313\n",
      "2.830106\n",
      "2.819022\n",
      "2.732417\n",
      "2.7458684\n",
      "2.7277324\n",
      "2.7249253\n",
      "2.6930015\n",
      "2.6497896\n",
      "2.6836078\n",
      "2.561738\n",
      "2.5338717\n",
      "2.4963238\n",
      "2.6208463\n",
      "2.508932\n",
      "2.514549\n",
      "2.422645\n",
      "2.4131181\n",
      "2.359625\n",
      "2.3814113\n",
      "2.1463122\n",
      "2.3268945\n",
      "2.434292\n",
      "2.1708136\n",
      "2.3460538\n",
      "2.1187754\n",
      "2.2181952\n",
      "2.1581728\n",
      "1.9968964\n",
      "2.0828059\n",
      "2.0462942\n",
      "2.0186296\n",
      "1.7370061\n",
      "1.6771889\n",
      "1.750224\n",
      "1.8745668\n",
      "1.5385408\n",
      "1.736633\n",
      "1.5342318\n",
      "1.2336196\n",
      "1.4817284\n",
      "1.7180281\n",
      "1.477541\n",
      "1.4616587\n",
      "1.2782574\n",
      "1.1241927\n",
      "1.3843576\n",
      "1.5579383\n",
      "1.2714173\n",
      "1.2576457\n",
      "1.2295457\n",
      "1.0905538\n",
      "1.2799876\n",
      "1.1036551\n",
      "1.076991\n",
      "1.3734944\n",
      "1.6947546\n",
      "1.3636255\n",
      "1.330712\n",
      "0.9183054\n",
      "1.2543826\n",
      "0.95442504\n",
      "0.8355622\n",
      "1.1141821\n",
      "1.0622885\n",
      "0.7416161\n",
      "0.7739282\n",
      "0.86074233\n",
      "0.74753934\n",
      "0.7468023\n",
      "0.9010777\n",
      "1.0444187\n",
      "0.8272786\n",
      "0.62195355\n",
      "1.0280789\n",
      "0.8651155\n",
      "1.1474582\n",
      "0.77188796\n",
      "0.5130211\n",
      "1.1460265\n",
      "0.6985978\n",
      "0.86331874\n",
      "1.0196549\n",
      "0.99700034\n",
      "0.8354189\n",
      "0.7107828\n",
      "0.7986897\n",
      "0.8646101\n",
      "0.7326718\n",
      "0.7868649\n",
      "0.7832988\n",
      "0.7207229\n",
      "0.88288957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ImprovedModel(Dense(22 => 128; bias=false), ARTransformerLayer[ARTransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(SwiGLU(Dense(128 => 512), Dense(128 => 512)), Dense(512 => 128)), RMSNorm{Float32, Vector{Float32}}(Float32[1.0202079, 1.0088698, 1.0088199, 1.0198886, 1.0157123, 1.0200112, 1.0007889, 1.0001472, 1.0067984, 1.029152  …  1.0158132, 1.0290358, 1.0039921, 1.0005008, 1.0163653, 1.0002795, 1.007354, 1.0051079, 1.0068612, 1.011573], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[1.0356601, 1.0193219, 1.0124823, 0.9961087, 1.0044967, 1.021329, 1.007686, 1.0234483, 1.0145389, 1.0226994  …  1.0038561, 1.0422041, 1.0196836, 1.0123327, 1.0100799, 1.0238266, 1.0175889, 1.0106137, 1.0085906, 1.0057552], 1.0f-5), Dropout(0.1)), ARTransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(SwiGLU(Dense(128 => 512), Dense(128 => 512)), Dense(512 => 128)), RMSNorm{Float32, Vector{Float32}}(Float32[1.0151058, 1.0068042, 0.99788666, 1.0030322, 0.9967731, 1.0005879, 0.9976334, 1.0028014, 1.008074, 0.9973706  …  1.0056317, 1.0051073, 0.99828935, 0.9888554, 0.9989846, 1.008264, 0.99682957, 0.9905077, 0.9803493, 0.9948587], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[1.0074768, 1.0026889, 1.0060511, 0.99273026, 1.0031543, 1.0046717, 0.9856305, 0.99489564, 0.98170376, 1.0129089  …  0.9952154, 0.9982427, 0.9893203, 0.99581736, 1.0100307, 1.0001254, 0.9900098, 0.9997926, 0.99776286, 1.0145509], 1.0f-5), Dropout(0.1)), ARTransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(SwiGLU(Dense(128 => 512), Dense(128 => 512)), Dense(512 => 128)), RMSNorm{Float32, Vector{Float32}}(Float32[0.99330693, 0.9986845, 1.0106893, 1.0017266, 1.002108, 1.0096269, 1.0134103, 1.027137, 1.001584, 1.0082177  …  0.99485815, 1.0059063, 1.0074261, 1.0035027, 0.99793535, 1.0163441, 1.0085806, 0.9930375, 1.0085508, 1.0142523], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[0.9932661, 1.0094172, 1.0045875, 0.99604386, 1.0084594, 1.0019063, 0.99843353, 0.99888986, 1.000727, 0.9933313  …  0.9991376, 0.9996906, 0.9987057, 0.98192644, 1.0056119, 1.0056579, 0.9988695, 1.0082902, 1.0191783, 0.9946275], 1.0f-5), Dropout(0.1)), ARTransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(SwiGLU(Dense(128 => 512), Dense(128 => 512)), Dense(512 => 128)), RMSNorm{Float32, Vector{Float32}}(Float32[1.0160056, 0.9921713, 1.0393482, 1.0008675, 0.99801177, 1.0130872, 1.0113844, 1.0287204, 1.0229939, 0.9942792  …  0.99634075, 1.0044357, 1.01033, 1.000195, 0.9929065, 1.025912, 1.0111202, 1.0064894, 1.0195317, 0.99800354], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[0.9924455, 0.9939411, 0.9955493, 1.002694, 1.0156876, 1.000124, 0.998056, 0.98768634, 0.99741083, 0.9946182  …  1.0068238, 0.99560827, 1.0017128, 1.0103025, 1.0100249, 0.992601, 0.9982385, 0.99456614, 1.0001156, 0.9919361], 1.0f-5), Dropout(0.1)), ARTransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(SwiGLU(Dense(128 => 512), Dense(128 => 512)), Dense(512 => 128)), RMSNorm{Float32, Vector{Float32}}(Float32[0.99243546, 1.0014242, 0.996402, 1.0244813, 1.0185994, 1.0011259, 1.0181508, 1.020127, 0.9992968, 1.0181859  …  0.9927547, 1.0056814, 1.000608, 0.9972028, 0.9917368, 1.0191554, 1.0014998, 0.9932919, 0.9958522, 0.99733895], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[0.99991643, 0.99244404, 1.0087222, 1.0018201, 0.9968345, 1.005614, 0.99889225, 1.0053207, 1.005301, 1.0046924  …  0.9915558, 0.99426526, 1.0027701, 1.007683, 0.99271053, 0.99925226, 0.9928893, 1.0090487, 1.0020236, 0.9861366], 1.0f-5), Dropout(0.1)), ARTransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(SwiGLU(Dense(128 => 512), Dense(128 => 512)), Dense(512 => 128)), RMSNorm{Float32, Vector{Float32}}(Float32[0.99696666, 1.0031027, 1.0153519, 0.9943336, 1.0091403, 1.0285051, 1.027018, 1.0048358, 1.0079137, 1.0008917  …  0.9963483, 0.99769115, 1.0099046, 1.0184263, 0.9980593, 1.0101936, 1.0061941, 1.0123706, 1.0071627, 1.0000165], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[0.9986133, 0.99676454, 1.0014324, 1.0043552, 1.0212662, 0.9939088, 1.0010989, 1.02255, 1.0025029, 0.99323404  …  1.0034591, 1.0049521, 1.0154823, 0.99890983, 1.00663, 1.0076922, 0.9904327, 0.994248, 1.013169, 1.006755], 1.0f-5), Dropout(0.1)), ARTransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(SwiGLU(Dense(128 => 512), Dense(128 => 512)), Dense(512 => 128)), RMSNorm{Float32, Vector{Float32}}(Float32[1.0129459, 1.0075482, 0.9888761, 1.0096625, 1.0126886, 1.0169245, 1.0126384, 1.0262356, 1.0124242, 1.0070533  …  1.021785, 0.9908767, 1.0137281, 0.99859595, 1.0103011, 1.0198641, 1.0051414, 1.0077281, 1.0080707, 0.9974648], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[0.99566364, 0.9893667, 1.0031521, 1.0076445, 1.0143653, 0.99189895, 0.9972018, 1.0160234, 1.0010362, 0.9944717  …  1.0020589, 1.0089843, 0.98216933, 0.9909217, 0.99863726, 0.9950435, 0.9932838, 1.010459, 1.0117092, 1.0017151], 1.0f-5), Dropout(0.1)), ARTransformerLayer(Attention{Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}, Dense{typeof(identity), Matrix{Float32}, Bool}}(Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), Dense(128 => 128; bias=false), 128, 2, 2, 64), Chain(SwiGLU(Dense(128 => 512), Dense(128 => 512)), Dense(512 => 128)), RMSNorm{Float32, Vector{Float32}}(Float32[1.0108995, 0.9938314, 1.006465, 1.0076711, 0.99672526, 1.0149404, 1.0044497, 1.0112417, 1.0071685, 1.0094837  …  1.022352, 1.0129063, 1.0082155, 1.0098085, 0.99843085, 1.0008303, 0.9976909, 1.0061958, 1.0187271, 0.99500066], 1.0f-5), RMSNorm{Float32, Vector{Float32}}(Float32[0.99358577, 0.9888837, 1.0218064, 0.99642867, 1.0031562, 0.99897105, 0.994452, 1.0180533, 0.99283206, 0.98973  …  0.99670315, 0.99794513, 0.99470365, 0.996865, 1.0037757, 0.9924296, 0.9974783, 1.0089387, 1.0040025, 0.99491274], 1.0f-5), Dropout(0.1))], Chain(RMSNorm{Float32, Vector{Float32}}(Float32[0.9935587, 0.9970702, 1.001908, 1.0040787, 0.9962857, 1.0058461, 0.99918246, 1.0030242, 0.99506855, 1.000426  …  0.99467826, 1.0014608, 1.0151315, 0.9982213, 1.0097772, 1.0045445, 0.99765855, 1.0035536, 0.9985324, 1.0084195], 1.0f-5), Dense(128 => 22))), Any[3.9736392f0, 3.827649f0, 3.7222044f0, 3.2871213f0, 3.0097704f0, 2.8940656f0, 2.8884637f0, 2.8412313f0, 2.830106f0, 2.819022f0  …  0.99700034f0, 0.8354189f0, 0.7107828f0, 0.7986897f0, 0.8646101f0, 0.7326718f0, 0.7868649f0, 0.7832988f0, 0.7207229f0, 0.88288957f0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, improved_loss = train_ar_model(model, batches[1:100], opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip470\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip470)\" d=\"M0 1600 L2400 1600 L2400 8.88178e-14 L0 8.88178e-14  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip471\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip470)\" d=\"M112.177 1486.45 L2352.76 1486.45 L2352.76 47.2441 L112.177 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip472\">\n",
       "    <rect x=\"112\" y=\"47\" width=\"2242\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"154.239,1486.45 154.239,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"688.015,1486.45 688.015,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1221.79,1486.45 1221.79,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1755.57,1486.45 1755.57,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2289.34,1486.45 2289.34,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"112.177,1254.65 2352.76,1254.65 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"112.177,862.314 2352.76,862.314 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"112.177,469.974 2352.76,469.974 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"112.177,77.6339 2352.76,77.6339 \"/>\n",
       "<polyline clip-path=\"url(#clip470)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"112.177,1486.45 2352.76,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip470)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"154.239,1486.45 154.239,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip470)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"688.015,1486.45 688.015,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip470)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1221.79,1486.45 1221.79,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip470)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1755.57,1486.45 1755.57,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip470)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2289.34,1486.45 2289.34,1467.55 \"/>\n",
       "<path clip-path=\"url(#clip470)\" d=\"M154.239 1517.37 Q150.628 1517.37 148.799 1520.93 Q146.993 1524.47 146.993 1531.6 Q146.993 1538.71 148.799 1542.27 Q150.628 1545.82 154.239 1545.82 Q157.873 1545.82 159.679 1542.27 Q161.507 1538.71 161.507 1531.6 Q161.507 1524.47 159.679 1520.93 Q157.873 1517.37 154.239 1517.37 M154.239 1513.66 Q160.049 1513.66 163.104 1518.27 Q166.183 1522.85 166.183 1531.6 Q166.183 1540.33 163.104 1544.94 Q160.049 1549.52 154.239 1549.52 Q148.429 1549.52 145.35 1544.94 Q142.294 1540.33 142.294 1531.6 Q142.294 1522.85 145.35 1518.27 Q148.429 1513.66 154.239 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M667.286 1544.91 L683.605 1544.91 L683.605 1548.85 L661.661 1548.85 L661.661 1544.91 Q664.323 1542.16 668.906 1537.53 Q673.513 1532.88 674.693 1531.53 Q676.939 1529.01 677.818 1527.27 Q678.721 1525.51 678.721 1523.82 Q678.721 1521.07 676.777 1519.33 Q674.855 1517.6 671.753 1517.6 Q669.554 1517.6 667.101 1518.36 Q664.67 1519.13 661.892 1520.68 L661.892 1515.95 Q664.716 1514.82 667.17 1514.24 Q669.624 1513.66 671.661 1513.66 Q677.031 1513.66 680.226 1516.35 Q683.42 1519.03 683.42 1523.52 Q683.42 1525.65 682.61 1527.57 Q681.823 1529.47 679.716 1532.07 Q679.138 1532.74 676.036 1535.95 Q672.934 1539.15 667.286 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M693.466 1514.29 L711.823 1514.29 L711.823 1518.22 L697.749 1518.22 L697.749 1526.7 Q698.767 1526.35 699.786 1526.19 Q700.804 1526 701.823 1526 Q707.61 1526 710.989 1529.17 Q714.369 1532.34 714.369 1537.76 Q714.369 1543.34 710.897 1546.44 Q707.424 1549.52 701.105 1549.52 Q698.929 1549.52 696.661 1549.15 Q694.415 1548.78 692.008 1548.04 L692.008 1543.34 Q694.091 1544.47 696.313 1545.03 Q698.536 1545.58 701.012 1545.58 Q705.017 1545.58 707.355 1543.48 Q709.693 1541.37 709.693 1537.76 Q709.693 1534.15 707.355 1532.04 Q705.017 1529.94 701.012 1529.94 Q699.138 1529.94 697.263 1530.35 Q695.411 1530.77 693.466 1531.65 L693.466 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M1196.49 1514.29 L1214.85 1514.29 L1214.85 1518.22 L1200.77 1518.22 L1200.77 1526.7 Q1201.79 1526.35 1202.81 1526.19 Q1203.83 1526 1204.85 1526 Q1210.63 1526 1214.01 1529.17 Q1217.39 1532.34 1217.39 1537.76 Q1217.39 1543.34 1213.92 1546.44 Q1210.45 1549.52 1204.13 1549.52 Q1201.95 1549.52 1199.68 1549.15 Q1197.44 1548.78 1195.03 1548.04 L1195.03 1543.34 Q1197.12 1544.47 1199.34 1545.03 Q1201.56 1545.58 1204.04 1545.58 Q1208.04 1545.58 1210.38 1543.48 Q1212.72 1541.37 1212.72 1537.76 Q1212.72 1534.15 1210.38 1532.04 Q1208.04 1529.94 1204.04 1529.94 Q1202.16 1529.94 1200.29 1530.35 Q1198.43 1530.77 1196.49 1531.65 L1196.49 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M1236.61 1517.37 Q1232.99 1517.37 1231.17 1520.93 Q1229.36 1524.47 1229.36 1531.6 Q1229.36 1538.71 1231.17 1542.27 Q1232.99 1545.82 1236.61 1545.82 Q1240.24 1545.82 1242.05 1542.27 Q1243.87 1538.71 1243.87 1531.6 Q1243.87 1524.47 1242.05 1520.93 Q1240.24 1517.37 1236.61 1517.37 M1236.61 1513.66 Q1242.42 1513.66 1245.47 1518.27 Q1248.55 1522.85 1248.55 1531.6 Q1248.55 1540.33 1245.47 1544.94 Q1242.42 1549.52 1236.61 1549.52 Q1230.8 1549.52 1227.72 1544.94 Q1224.66 1540.33 1224.66 1531.6 Q1224.66 1522.85 1227.72 1518.27 Q1230.8 1513.66 1236.61 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M1729.42 1514.29 L1751.64 1514.29 L1751.64 1516.28 L1739.1 1548.85 L1734.21 1548.85 L1746.02 1518.22 L1729.42 1518.22 L1729.42 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M1760.81 1514.29 L1779.17 1514.29 L1779.17 1518.22 L1765.09 1518.22 L1765.09 1526.7 Q1766.11 1526.35 1767.13 1526.19 Q1768.15 1526 1769.17 1526 Q1774.95 1526 1778.33 1529.17 Q1781.71 1532.34 1781.71 1537.76 Q1781.71 1543.34 1778.24 1546.44 Q1774.77 1549.52 1768.45 1549.52 Q1766.27 1549.52 1764 1549.15 Q1761.76 1548.78 1759.35 1548.04 L1759.35 1543.34 Q1761.44 1544.47 1763.66 1545.03 Q1765.88 1545.58 1768.36 1545.58 Q1772.36 1545.58 1774.7 1543.48 Q1777.04 1541.37 1777.04 1537.76 Q1777.04 1534.15 1774.7 1532.04 Q1772.36 1529.94 1768.36 1529.94 Q1766.48 1529.94 1764.61 1530.35 Q1762.75 1530.77 1760.81 1531.65 L1760.81 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2248.95 1544.91 L2256.59 1544.91 L2256.59 1518.55 L2248.28 1520.21 L2248.28 1515.95 L2256.54 1514.29 L2261.22 1514.29 L2261.22 1544.91 L2268.86 1544.91 L2268.86 1548.85 L2248.95 1548.85 L2248.95 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2288.3 1517.37 Q2284.69 1517.37 2282.86 1520.93 Q2281.06 1524.47 2281.06 1531.6 Q2281.06 1538.71 2282.86 1542.27 Q2284.69 1545.82 2288.3 1545.82 Q2291.94 1545.82 2293.74 1542.27 Q2295.57 1538.71 2295.57 1531.6 Q2295.57 1524.47 2293.74 1520.93 Q2291.94 1517.37 2288.3 1517.37 M2288.3 1513.66 Q2294.11 1513.66 2297.17 1518.27 Q2300.25 1522.85 2300.25 1531.6 Q2300.25 1540.33 2297.17 1544.94 Q2294.11 1549.52 2288.3 1549.52 Q2282.49 1549.52 2279.41 1544.94 Q2276.36 1540.33 2276.36 1531.6 Q2276.36 1522.85 2279.41 1518.27 Q2282.49 1513.66 2288.3 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2318.46 1517.37 Q2314.85 1517.37 2313.02 1520.93 Q2311.22 1524.47 2311.22 1531.6 Q2311.22 1538.71 2313.02 1542.27 Q2314.85 1545.82 2318.46 1545.82 Q2322.1 1545.82 2323.9 1542.27 Q2325.73 1538.71 2325.73 1531.6 Q2325.73 1524.47 2323.9 1520.93 Q2322.1 1517.37 2318.46 1517.37 M2318.46 1513.66 Q2324.27 1513.66 2327.33 1518.27 Q2330.41 1522.85 2330.41 1531.6 Q2330.41 1540.33 2327.33 1544.94 Q2324.27 1549.52 2318.46 1549.52 Q2312.65 1549.52 2309.57 1544.94 Q2306.52 1540.33 2306.52 1531.6 Q2306.52 1522.85 2309.57 1518.27 Q2312.65 1513.66 2318.46 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip470)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"112.177,1486.45 112.177,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip470)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"112.177,1254.65 131.075,1254.65 \"/>\n",
       "<polyline clip-path=\"url(#clip470)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"112.177,862.314 131.075,862.314 \"/>\n",
       "<polyline clip-path=\"url(#clip470)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"112.177,469.974 131.075,469.974 \"/>\n",
       "<polyline clip-path=\"url(#clip470)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"112.177,77.6339 131.075,77.6339 \"/>\n",
       "<path clip-path=\"url(#clip470)\" d=\"M56.2699 1268 L63.9087 1268 L63.9087 1241.63 L55.5986 1243.3 L55.5986 1239.04 L63.8624 1237.37 L68.5383 1237.37 L68.5383 1268 L76.1772 1268 L76.1772 1271.93 L56.2699 1271.93 L56.2699 1268 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M59.8578 875.659 L76.1772 875.659 L76.1772 879.594 L54.2328 879.594 L54.2328 875.659 Q56.8949 872.904 61.4782 868.275 Q66.0846 863.622 67.2652 862.279 Q69.5105 859.756 70.3902 858.02 Q71.2929 856.261 71.2929 854.571 Q71.2929 851.817 69.3485 850.08 Q67.4272 848.344 64.3254 848.344 Q62.1263 848.344 59.6726 849.108 Q57.2421 849.872 54.4643 851.423 L54.4643 846.701 Q57.2884 845.567 59.7421 844.988 Q62.1958 844.409 64.2328 844.409 Q69.6031 844.409 72.7976 847.094 Q75.992 849.78 75.992 854.27 Q75.992 856.4 75.1818 858.321 Q74.3948 860.219 72.2883 862.812 Q71.7096 863.483 68.6078 866.701 Q65.5059 869.895 59.8578 875.659 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M69.0476 468.62 Q72.404 469.337 74.279 471.606 Q76.1772 473.874 76.1772 477.208 Q76.1772 482.324 72.6587 485.124 Q69.1402 487.925 62.6587 487.925 Q60.4828 487.925 58.168 487.486 Q55.8764 487.069 53.4227 486.212 L53.4227 481.699 Q55.3671 482.833 57.6819 483.411 Q59.9967 483.99 62.5198 483.99 Q66.918 483.99 69.2096 482.254 Q71.5244 480.518 71.5244 477.208 Q71.5244 474.152 69.3717 472.439 Q67.242 470.703 63.4226 470.703 L59.3949 470.703 L59.3949 466.861 L63.6078 466.861 Q67.0569 466.861 68.8855 465.495 Q70.7142 464.106 70.7142 461.513 Q70.7142 458.851 68.8161 457.439 Q66.9411 456.004 63.4226 456.004 Q61.5013 456.004 59.3023 456.421 Q57.1032 456.838 54.4643 457.717 L54.4643 453.551 Q57.1264 452.81 59.4412 452.439 Q61.7791 452.069 63.8393 452.069 Q69.1633 452.069 72.2652 454.5 Q75.367 456.907 75.367 461.027 Q75.367 463.898 73.7235 465.888 Q72.08 467.856 69.0476 468.62 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M66.5939 64.4279 L54.7884 82.8769 L66.5939 82.8769 L66.5939 64.4279 M65.367 60.3539 L71.2466 60.3539 L71.2466 82.8769 L76.1772 82.8769 L76.1772 86.7658 L71.2466 86.7658 L71.2466 94.9139 L66.5939 94.9139 L66.5939 86.7658 L50.9921 86.7658 L50.9921 82.2519 L65.367 60.3539 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip472)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"175.59,87.9763 196.941,145.254 218.292,186.624 239.643,357.325 260.994,466.141 282.345,511.536 303.696,513.734 325.047,532.265 346.398,536.63 367.749,540.979 389.1,574.958 410.451,569.68 431.802,576.796 453.153,577.897 474.504,590.422 495.855,607.376 517.207,594.107 538.558,641.922 559.909,652.855 581.26,667.586 602.611,618.731 623.962,662.64 645.313,660.436 666.664,696.494 688.015,700.231 709.366,721.219 730.717,712.671 752.068,804.91 773.419,734.06 794.77,691.924 816.121,795.297 837.472,726.543 858.823,815.714 880.174,776.707 901.525,800.257 922.876,863.532 944.227,829.826 965.578,844.151 986.93,855.005 1008.28,965.497 1029.63,988.966 1050.98,960.311 1072.33,911.527 1093.68,1043.36 1115.04,965.644 1136.39,1045.05 1157.74,1163 1179.09,1065.65 1200.44,972.943 1221.79,1067.3 1243.14,1073.53 1264.49,1145.48 1285.84,1205.93 1307.2,1103.86 1328.55,1035.75 1349.9,1148.17 1371.25,1153.57 1392.6,1164.59 1413.95,1219.13 1435.3,1144.8 1456.65,1213.99 1478,1224.45 1499.35,1108.12 1520.71,982.074 1542.06,1111.99 1563.41,1124.9 1584.76,1286.71 1606.11,1154.85 1627.46,1272.54 1648.81,1319.17 1670.16,1209.86 1691.51,1230.22 1712.87,1356.03 1734.22,1343.35 1755.57,1309.29 1776.92,1353.7 1798.27,1353.99 1819.62,1293.47 1840.97,1237.23 1862.32,1322.42 1883.67,1402.98 1905.02,1243.64 1926.38,1307.57 1947.73,1196.8 1969.08,1344.15 1990.43,1445.72 2011.78,1197.36 2033.13,1372.91 2054.48,1308.28 2075.83,1246.94 2097.18,1255.83 2118.53,1319.23 2139.89,1368.13 2161.24,1333.64 2182.59,1307.77 2203.94,1359.54 2225.29,1338.28 2246.64,1339.67 2267.99,1364.23 2289.34,1300.6 \"/>\n",
       "<polyline clip-path=\"url(#clip472)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"175.59,434.262 196.941,443.538 218.292,193.28 239.643,457.815 260.994,441.87 282.345,442.45 303.696,445.458 325.047,452.826 346.398,471.58 367.749,505.106 389.1,409.728 410.451,508.643 431.802,481.557 453.153,467.304 474.504,465.881 495.855,465.662 517.207,465.679 538.558,471.755 559.909,479.374 581.26,482.779 602.611,479.745 623.962,483.562 645.313,495.432 666.664,515.036 688.015,521.544 709.366,505.544 730.717,522.781 752.068,521.86 773.419,514.536 794.77,514.407 816.121,525.948 837.472,518.436 858.823,526.64 880.174,515.059 901.525,525.847 922.876,523.333 944.227,526.53 965.578,523.321 986.93,512.698 1008.28,522.729 1029.63,518.82 1050.98,523.293 1072.33,510.997 1093.68,524.62 1115.04,523.009 1136.39,522.406 1157.74,520.885 1179.09,524.648 1200.44,524.999 1221.79,523.683 1243.14,517.961 1264.49,522.797 1285.84,537.869 1307.2,520.573 1328.55,515.908 1349.9,525.438 1371.25,510.962 1392.6,523.203 1413.95,528.105 1435.3,516.896 1456.65,517.213 1478,530.146 1499.35,523.821 1520.71,509.623 1542.06,509.809 1563.41,519.579 1584.76,524.588 1606.11,523.886 1627.46,523.789 1648.81,526.23 1670.16,519.555 1691.51,513.631 1712.87,531.587 1734.22,516.411 1755.57,520.809 1776.92,528.535 1798.27,519.264 1819.62,530.552 1840.97,507.84 1862.32,517.883 1883.67,526.389 1905.02,517.809 1926.38,525.512 1947.73,517.962 1969.08,517.01 1990.43,519.791 2011.78,516.063 2033.13,514.863 2054.48,508.905 2075.83,522.682 2097.18,531.22 2118.53,521.78 2139.89,519.955 2161.24,518.605 2182.59,536.067 2203.94,539.963 2225.29,517.277 2246.64,526.469 2267.99,520.274 2289.34,532.912 \"/>\n",
       "<path clip-path=\"url(#clip470)\" d=\"M1729.94 250.738 L2278.07 250.738 L2278.07 95.2176 L1729.94 95.2176  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip470)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1729.94,250.738 2278.07,250.738 2278.07,95.2176 1729.94,95.2176 1729.94,250.738 \"/>\n",
       "<polyline clip-path=\"url(#clip470)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1754.84,147.058 1904.21,147.058 \"/>\n",
       "<path clip-path=\"url(#clip470)\" d=\"M1929.1 138.412 L1933.36 138.412 L1933.36 164.338 L1929.1 164.338 L1929.1 138.412 M1929.1 128.319 L1933.36 128.319 L1933.36 133.713 L1929.1 133.713 L1929.1 128.319 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M1962.46 143.389 Q1964.06 140.518 1966.28 139.153 Q1968.5 137.787 1971.51 137.787 Q1975.56 137.787 1977.76 140.634 Q1979.96 143.458 1979.96 148.689 L1979.96 164.338 L1975.68 164.338 L1975.68 148.828 Q1975.68 145.102 1974.36 143.296 Q1973.04 141.49 1970.33 141.49 Q1967.02 141.49 1965.1 143.69 Q1963.18 145.889 1963.18 149.685 L1963.18 164.338 L1958.89 164.338 L1958.89 148.828 Q1958.89 145.078 1957.57 143.296 Q1956.25 141.49 1953.5 141.49 Q1950.24 141.49 1948.32 143.713 Q1946.39 145.912 1946.39 149.685 L1946.39 164.338 L1942.11 164.338 L1942.11 138.412 L1946.39 138.412 L1946.39 142.44 Q1947.85 140.055 1949.89 138.921 Q1951.93 137.787 1954.73 137.787 Q1957.55 137.787 1959.52 139.222 Q1961.51 140.657 1962.46 143.389 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M1992.57 160.449 L1992.57 174.199 L1988.29 174.199 L1988.29 138.412 L1992.57 138.412 L1992.57 142.347 Q1993.92 140.032 1995.95 138.921 Q1998.01 137.787 2000.86 137.787 Q2005.58 137.787 2008.52 141.537 Q2011.49 145.287 2011.49 151.398 Q2011.49 157.509 2008.52 161.259 Q2005.58 165.009 2000.86 165.009 Q1998.01 165.009 1995.95 163.898 Q1993.92 162.763 1992.57 160.449 M2007.06 151.398 Q2007.06 146.699 2005.12 144.037 Q2003.2 141.352 1999.82 141.352 Q1996.44 141.352 1994.5 144.037 Q1992.57 146.699 1992.57 151.398 Q1992.57 156.097 1994.5 158.782 Q1996.44 161.444 1999.82 161.444 Q2003.2 161.444 2005.12 158.782 Q2007.06 156.097 2007.06 151.398 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2033.57 142.393 Q2032.85 141.977 2032 141.791 Q2031.16 141.583 2030.14 141.583 Q2026.53 141.583 2024.59 143.944 Q2022.67 146.282 2022.67 150.68 L2022.67 164.338 L2018.38 164.338 L2018.38 138.412 L2022.67 138.412 L2022.67 142.44 Q2024.01 140.078 2026.16 138.944 Q2028.31 137.787 2031.39 137.787 Q2031.83 137.787 2032.37 137.856 Q2032.9 137.903 2033.55 138.018 L2033.57 142.393 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2047.04 141.398 Q2043.62 141.398 2041.62 144.083 Q2039.63 146.745 2039.63 151.398 Q2039.63 156.051 2041.6 158.736 Q2043.59 161.398 2047.04 161.398 Q2050.44 161.398 2052.43 158.713 Q2054.43 156.027 2054.43 151.398 Q2054.43 146.791 2052.43 144.106 Q2050.44 141.398 2047.04 141.398 M2047.04 137.787 Q2052.6 137.787 2055.77 141.398 Q2058.94 145.009 2058.94 151.398 Q2058.94 157.764 2055.77 161.398 Q2052.6 165.009 2047.04 165.009 Q2041.46 165.009 2038.29 161.398 Q2035.14 157.764 2035.14 151.398 Q2035.14 145.009 2038.29 141.398 Q2041.46 137.787 2047.04 137.787 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2062.94 138.412 L2067.46 138.412 L2075.56 160.171 L2083.66 138.412 L2088.18 138.412 L2078.45 164.338 L2072.67 164.338 L2062.94 138.412 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2116.23 150.31 L2116.23 152.393 L2096.65 152.393 Q2096.93 156.791 2099.29 159.106 Q2101.67 161.398 2105.91 161.398 Q2108.36 161.398 2110.65 160.796 Q2112.97 160.194 2115.24 158.99 L2115.24 163.018 Q2112.94 163.99 2110.54 164.5 Q2108.13 165.009 2105.65 165.009 Q2099.45 165.009 2095.81 161.398 Q2092.2 157.787 2092.2 151.629 Q2092.2 145.264 2095.63 141.537 Q2099.08 137.787 2104.91 137.787 Q2110.14 137.787 2113.18 141.166 Q2116.23 144.523 2116.23 150.31 M2111.97 149.06 Q2111.93 145.565 2110 143.481 Q2108.11 141.398 2104.96 141.398 Q2101.39 141.398 2099.24 143.412 Q2097.11 145.426 2096.79 149.083 L2111.97 149.06 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2140.28 142.347 L2140.28 128.319 L2144.54 128.319 L2144.54 164.338 L2140.28 164.338 L2140.28 160.449 Q2138.94 162.763 2136.88 163.898 Q2134.84 165.009 2131.97 165.009 Q2127.27 165.009 2124.31 161.259 Q2121.37 157.509 2121.37 151.398 Q2121.37 145.287 2124.31 141.537 Q2127.27 137.787 2131.97 137.787 Q2134.84 137.787 2136.88 138.921 Q2138.94 140.032 2140.28 142.347 M2125.77 151.398 Q2125.77 156.097 2127.69 158.782 Q2129.63 161.444 2133.01 161.444 Q2136.39 161.444 2138.34 158.782 Q2140.28 156.097 2140.28 151.398 Q2140.28 146.699 2138.34 144.037 Q2136.39 141.352 2133.01 141.352 Q2129.63 141.352 2127.69 144.037 Q2125.77 146.699 2125.77 151.398 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2168.38 128.319 L2172.64 128.319 L2172.64 164.338 L2168.38 164.338 L2168.38 128.319 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2191.6 141.398 Q2188.17 141.398 2186.18 144.083 Q2184.19 146.745 2184.19 151.398 Q2184.19 156.051 2186.16 158.736 Q2188.15 161.398 2191.6 161.398 Q2195 161.398 2196.99 158.713 Q2198.99 156.027 2198.99 151.398 Q2198.99 146.791 2196.99 144.106 Q2195 141.398 2191.6 141.398 M2191.6 137.787 Q2197.16 137.787 2200.33 141.398 Q2203.5 145.009 2203.5 151.398 Q2203.5 157.764 2200.33 161.398 Q2197.16 165.009 2191.6 165.009 Q2186.02 165.009 2182.85 161.398 Q2179.7 157.764 2179.7 151.398 Q2179.7 145.009 2182.85 141.398 Q2186.02 137.787 2191.6 137.787 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2227.09 139.176 L2227.09 143.203 Q2225.28 142.277 2223.34 141.815 Q2221.39 141.352 2219.31 141.352 Q2216.14 141.352 2214.54 142.324 Q2212.97 143.296 2212.97 145.24 Q2212.97 146.722 2214.1 147.578 Q2215.24 148.412 2218.66 149.176 L2220.12 149.5 Q2224.66 150.472 2226.55 152.254 Q2228.48 154.014 2228.48 157.185 Q2228.48 160.796 2225.61 162.902 Q2222.76 165.009 2217.76 165.009 Q2215.67 165.009 2213.41 164.592 Q2211.16 164.199 2208.66 163.388 L2208.66 158.99 Q2211.02 160.217 2213.31 160.842 Q2215.61 161.444 2217.85 161.444 Q2220.86 161.444 2222.48 160.426 Q2224.1 159.384 2224.1 157.509 Q2224.1 155.773 2222.92 154.847 Q2221.76 153.921 2217.8 153.064 L2216.32 152.717 Q2212.36 151.884 2210.61 150.171 Q2208.85 148.435 2208.85 145.426 Q2208.85 141.768 2211.44 139.778 Q2214.03 137.787 2218.8 137.787 Q2221.16 137.787 2223.24 138.134 Q2225.33 138.481 2227.09 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2251.79 139.176 L2251.79 143.203 Q2249.98 142.277 2248.04 141.815 Q2246.09 141.352 2244.01 141.352 Q2240.84 141.352 2239.24 142.324 Q2237.67 143.296 2237.67 145.24 Q2237.67 146.722 2238.8 147.578 Q2239.93 148.412 2243.36 149.176 L2244.82 149.5 Q2249.36 150.472 2251.25 152.254 Q2253.17 154.014 2253.17 157.185 Q2253.17 160.796 2250.3 162.902 Q2247.46 165.009 2242.46 165.009 Q2240.37 165.009 2238.11 164.592 Q2235.86 164.199 2233.36 163.388 L2233.36 158.99 Q2235.72 160.217 2238.01 160.842 Q2240.3 161.444 2242.55 161.444 Q2245.56 161.444 2247.18 160.426 Q2248.8 159.384 2248.8 157.509 Q2248.8 155.773 2247.62 154.847 Q2246.46 153.921 2242.5 153.064 L2241.02 152.717 Q2237.06 151.884 2235.3 150.171 Q2233.55 148.435 2233.55 145.426 Q2233.55 141.768 2236.14 139.778 Q2238.73 137.787 2243.5 137.787 Q2245.86 137.787 2247.94 138.134 Q2250.03 138.481 2251.79 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip470)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1754.84,198.898 1904.21,198.898 \"/>\n",
       "<path clip-path=\"url(#clip470)\" d=\"M1936.51 182.891 L1936.51 190.252 L1945.28 190.252 L1945.28 193.562 L1936.51 193.562 L1936.51 207.636 Q1936.51 210.807 1937.37 211.71 Q1938.25 212.613 1940.91 212.613 L1945.28 212.613 L1945.28 216.178 L1940.91 216.178 Q1935.98 216.178 1934.1 214.349 Q1932.23 212.497 1932.23 207.636 L1932.23 193.562 L1929.1 193.562 L1929.1 190.252 L1932.23 190.252 L1932.23 182.891 L1936.51 182.891 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M1960.93 193.238 Q1957.5 193.238 1955.51 195.923 Q1953.52 198.585 1953.52 203.238 Q1953.52 207.891 1955.49 210.576 Q1957.48 213.238 1960.93 213.238 Q1964.33 213.238 1966.32 210.553 Q1968.32 207.867 1968.32 203.238 Q1968.32 198.631 1966.32 195.946 Q1964.33 193.238 1960.93 193.238 M1960.93 189.627 Q1966.49 189.627 1969.66 193.238 Q1972.83 196.849 1972.83 203.238 Q1972.83 209.604 1969.66 213.238 Q1966.49 216.849 1960.93 216.849 Q1955.35 216.849 1952.18 213.238 Q1949.03 209.604 1949.03 203.238 Q1949.03 196.849 1952.18 193.238 Q1955.35 189.627 1960.93 189.627 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M1990.68 218.585 Q1988.87 223.215 1987.16 224.627 Q1985.44 226.039 1982.57 226.039 L1979.17 226.039 L1979.17 222.474 L1981.67 222.474 Q1983.43 222.474 1984.4 221.64 Q1985.38 220.807 1986.56 217.705 L1987.32 215.761 L1976.83 190.252 L1981.35 190.252 L1989.45 210.529 L1997.55 190.252 L2002.06 190.252 L1990.68 218.585 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2023.01 180.159 L2027.27 180.159 L2027.27 216.178 L2023.01 216.178 L2023.01 180.159 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2046.23 193.238 Q2042.81 193.238 2040.81 195.923 Q2038.82 198.585 2038.82 203.238 Q2038.82 207.891 2040.79 210.576 Q2042.78 213.238 2046.23 213.238 Q2049.63 213.238 2051.62 210.553 Q2053.62 207.867 2053.62 203.238 Q2053.62 198.631 2051.62 195.946 Q2049.63 193.238 2046.23 193.238 M2046.23 189.627 Q2051.79 189.627 2054.96 193.238 Q2058.13 196.849 2058.13 203.238 Q2058.13 209.604 2054.96 213.238 Q2051.79 216.849 2046.23 216.849 Q2040.65 216.849 2037.48 213.238 Q2034.33 209.604 2034.33 203.238 Q2034.33 196.849 2037.48 193.238 Q2040.65 189.627 2046.23 189.627 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2081.72 191.016 L2081.72 195.043 Q2079.91 194.117 2077.97 193.655 Q2076.02 193.192 2073.94 193.192 Q2070.77 193.192 2069.17 194.164 Q2067.6 195.136 2067.6 197.08 Q2067.6 198.562 2068.73 199.418 Q2069.87 200.252 2073.29 201.016 L2074.75 201.34 Q2079.29 202.312 2081.18 204.094 Q2083.11 205.854 2083.11 209.025 Q2083.11 212.636 2080.24 214.742 Q2077.39 216.849 2072.39 216.849 Q2070.31 216.849 2068.04 216.432 Q2065.79 216.039 2063.29 215.228 L2063.29 210.83 Q2065.65 212.057 2067.94 212.682 Q2070.24 213.284 2072.48 213.284 Q2075.49 213.284 2077.11 212.266 Q2078.73 211.224 2078.73 209.349 Q2078.73 207.613 2077.55 206.687 Q2076.39 205.761 2072.43 204.904 L2070.95 204.557 Q2067 203.724 2065.24 202.011 Q2063.48 200.275 2063.48 197.266 Q2063.48 193.608 2066.07 191.618 Q2068.66 189.627 2073.43 189.627 Q2075.79 189.627 2077.87 189.974 Q2079.96 190.321 2081.72 191.016 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip470)\" d=\"M2106.42 191.016 L2106.42 195.043 Q2104.61 194.117 2102.67 193.655 Q2100.72 193.192 2098.64 193.192 Q2095.47 193.192 2093.87 194.164 Q2092.3 195.136 2092.3 197.08 Q2092.3 198.562 2093.43 199.418 Q2094.56 200.252 2097.99 201.016 L2099.45 201.34 Q2103.99 202.312 2105.88 204.094 Q2107.81 205.854 2107.81 209.025 Q2107.81 212.636 2104.93 214.742 Q2102.09 216.849 2097.09 216.849 Q2095 216.849 2092.74 216.432 Q2090.49 216.039 2087.99 215.228 L2087.99 210.83 Q2090.35 212.057 2092.64 212.682 Q2094.93 213.284 2097.18 213.284 Q2100.19 213.284 2101.81 212.266 Q2103.43 211.224 2103.43 209.349 Q2103.43 207.613 2102.25 206.687 Q2101.09 205.761 2097.13 204.904 L2095.65 204.557 Q2091.69 203.724 2089.93 202.011 Q2088.18 200.275 2088.18 197.266 Q2088.18 193.608 2090.77 191.618 Q2093.36 189.627 2098.13 189.627 Q2100.49 189.627 2102.57 189.974 Q2104.66 190.321 2106.42 191.016 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ],
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip520\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip520)\" d=\"M0 1600 L2400 1600 L2400 8.88178e-14 L0 8.88178e-14  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip521\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip520)\" d=\"M112.177 1486.45 L2352.76 1486.45 L2352.76 47.2441 L112.177 47.2441  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip522\">\n",
       "    <rect x=\"112\" y=\"47\" width=\"2242\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"154.239,1486.45 154.239,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"688.015,1486.45 688.015,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1221.79,1486.45 1221.79,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1755.57,1486.45 1755.57,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2289.34,1486.45 2289.34,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"112.177,1254.65 2352.76,1254.65 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"112.177,862.314 2352.76,862.314 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"112.177,469.974 2352.76,469.974 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"112.177,77.6339 2352.76,77.6339 \"/>\n",
       "<polyline clip-path=\"url(#clip520)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"112.177,1486.45 2352.76,1486.45 \"/>\n",
       "<polyline clip-path=\"url(#clip520)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"154.239,1486.45 154.239,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip520)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"688.015,1486.45 688.015,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip520)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1221.79,1486.45 1221.79,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip520)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1755.57,1486.45 1755.57,1467.55 \"/>\n",
       "<polyline clip-path=\"url(#clip520)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2289.34,1486.45 2289.34,1467.55 \"/>\n",
       "<path clip-path=\"url(#clip520)\" d=\"M154.239 1517.37 Q150.628 1517.37 148.799 1520.93 Q146.993 1524.47 146.993 1531.6 Q146.993 1538.71 148.799 1542.27 Q150.628 1545.82 154.239 1545.82 Q157.873 1545.82 159.679 1542.27 Q161.507 1538.71 161.507 1531.6 Q161.507 1524.47 159.679 1520.93 Q157.873 1517.37 154.239 1517.37 M154.239 1513.66 Q160.049 1513.66 163.104 1518.27 Q166.183 1522.85 166.183 1531.6 Q166.183 1540.33 163.104 1544.94 Q160.049 1549.52 154.239 1549.52 Q148.429 1549.52 145.35 1544.94 Q142.294 1540.33 142.294 1531.6 Q142.294 1522.85 145.35 1518.27 Q148.429 1513.66 154.239 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M667.286 1544.91 L683.605 1544.91 L683.605 1548.85 L661.661 1548.85 L661.661 1544.91 Q664.323 1542.16 668.906 1537.53 Q673.513 1532.88 674.693 1531.53 Q676.939 1529.01 677.818 1527.27 Q678.721 1525.51 678.721 1523.82 Q678.721 1521.07 676.777 1519.33 Q674.855 1517.6 671.753 1517.6 Q669.554 1517.6 667.101 1518.36 Q664.67 1519.13 661.892 1520.68 L661.892 1515.95 Q664.716 1514.82 667.17 1514.24 Q669.624 1513.66 671.661 1513.66 Q677.031 1513.66 680.226 1516.35 Q683.42 1519.03 683.42 1523.52 Q683.42 1525.65 682.61 1527.57 Q681.823 1529.47 679.716 1532.07 Q679.138 1532.74 676.036 1535.95 Q672.934 1539.15 667.286 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M693.466 1514.29 L711.823 1514.29 L711.823 1518.22 L697.749 1518.22 L697.749 1526.7 Q698.767 1526.35 699.786 1526.19 Q700.804 1526 701.823 1526 Q707.61 1526 710.989 1529.17 Q714.369 1532.34 714.369 1537.76 Q714.369 1543.34 710.897 1546.44 Q707.424 1549.52 701.105 1549.52 Q698.929 1549.52 696.661 1549.15 Q694.415 1548.78 692.008 1548.04 L692.008 1543.34 Q694.091 1544.47 696.313 1545.03 Q698.536 1545.58 701.012 1545.58 Q705.017 1545.58 707.355 1543.48 Q709.693 1541.37 709.693 1537.76 Q709.693 1534.15 707.355 1532.04 Q705.017 1529.94 701.012 1529.94 Q699.138 1529.94 697.263 1530.35 Q695.411 1530.77 693.466 1531.65 L693.466 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M1196.49 1514.29 L1214.85 1514.29 L1214.85 1518.22 L1200.77 1518.22 L1200.77 1526.7 Q1201.79 1526.35 1202.81 1526.19 Q1203.83 1526 1204.85 1526 Q1210.63 1526 1214.01 1529.17 Q1217.39 1532.34 1217.39 1537.76 Q1217.39 1543.34 1213.92 1546.44 Q1210.45 1549.52 1204.13 1549.52 Q1201.95 1549.52 1199.68 1549.15 Q1197.44 1548.78 1195.03 1548.04 L1195.03 1543.34 Q1197.12 1544.47 1199.34 1545.03 Q1201.56 1545.58 1204.04 1545.58 Q1208.04 1545.58 1210.38 1543.48 Q1212.72 1541.37 1212.72 1537.76 Q1212.72 1534.15 1210.38 1532.04 Q1208.04 1529.94 1204.04 1529.94 Q1202.16 1529.94 1200.29 1530.35 Q1198.43 1530.77 1196.49 1531.65 L1196.49 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M1236.61 1517.37 Q1232.99 1517.37 1231.17 1520.93 Q1229.36 1524.47 1229.36 1531.6 Q1229.36 1538.71 1231.17 1542.27 Q1232.99 1545.82 1236.61 1545.82 Q1240.24 1545.82 1242.05 1542.27 Q1243.87 1538.71 1243.87 1531.6 Q1243.87 1524.47 1242.05 1520.93 Q1240.24 1517.37 1236.61 1517.37 M1236.61 1513.66 Q1242.42 1513.66 1245.47 1518.27 Q1248.55 1522.85 1248.55 1531.6 Q1248.55 1540.33 1245.47 1544.94 Q1242.42 1549.52 1236.61 1549.52 Q1230.8 1549.52 1227.72 1544.94 Q1224.66 1540.33 1224.66 1531.6 Q1224.66 1522.85 1227.72 1518.27 Q1230.8 1513.66 1236.61 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M1729.42 1514.29 L1751.64 1514.29 L1751.64 1516.28 L1739.1 1548.85 L1734.21 1548.85 L1746.02 1518.22 L1729.42 1518.22 L1729.42 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M1760.81 1514.29 L1779.17 1514.29 L1779.17 1518.22 L1765.09 1518.22 L1765.09 1526.7 Q1766.11 1526.35 1767.13 1526.19 Q1768.15 1526 1769.17 1526 Q1774.95 1526 1778.33 1529.17 Q1781.71 1532.34 1781.71 1537.76 Q1781.71 1543.34 1778.24 1546.44 Q1774.77 1549.52 1768.45 1549.52 Q1766.27 1549.52 1764 1549.15 Q1761.76 1548.78 1759.35 1548.04 L1759.35 1543.34 Q1761.44 1544.47 1763.66 1545.03 Q1765.88 1545.58 1768.36 1545.58 Q1772.36 1545.58 1774.7 1543.48 Q1777.04 1541.37 1777.04 1537.76 Q1777.04 1534.15 1774.7 1532.04 Q1772.36 1529.94 1768.36 1529.94 Q1766.48 1529.94 1764.61 1530.35 Q1762.75 1530.77 1760.81 1531.65 L1760.81 1514.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2248.95 1544.91 L2256.59 1544.91 L2256.59 1518.55 L2248.28 1520.21 L2248.28 1515.95 L2256.54 1514.29 L2261.22 1514.29 L2261.22 1544.91 L2268.86 1544.91 L2268.86 1548.85 L2248.95 1548.85 L2248.95 1544.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2288.3 1517.37 Q2284.69 1517.37 2282.86 1520.93 Q2281.06 1524.47 2281.06 1531.6 Q2281.06 1538.71 2282.86 1542.27 Q2284.69 1545.82 2288.3 1545.82 Q2291.94 1545.82 2293.74 1542.27 Q2295.57 1538.71 2295.57 1531.6 Q2295.57 1524.47 2293.74 1520.93 Q2291.94 1517.37 2288.3 1517.37 M2288.3 1513.66 Q2294.11 1513.66 2297.17 1518.27 Q2300.25 1522.85 2300.25 1531.6 Q2300.25 1540.33 2297.17 1544.94 Q2294.11 1549.52 2288.3 1549.52 Q2282.49 1549.52 2279.41 1544.94 Q2276.36 1540.33 2276.36 1531.6 Q2276.36 1522.85 2279.41 1518.27 Q2282.49 1513.66 2288.3 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2318.46 1517.37 Q2314.85 1517.37 2313.02 1520.93 Q2311.22 1524.47 2311.22 1531.6 Q2311.22 1538.71 2313.02 1542.27 Q2314.85 1545.82 2318.46 1545.82 Q2322.1 1545.82 2323.9 1542.27 Q2325.73 1538.71 2325.73 1531.6 Q2325.73 1524.47 2323.9 1520.93 Q2322.1 1517.37 2318.46 1517.37 M2318.46 1513.66 Q2324.27 1513.66 2327.33 1518.27 Q2330.41 1522.85 2330.41 1531.6 Q2330.41 1540.33 2327.33 1544.94 Q2324.27 1549.52 2318.46 1549.52 Q2312.65 1549.52 2309.57 1544.94 Q2306.52 1540.33 2306.52 1531.6 Q2306.52 1522.85 2309.57 1518.27 Q2312.65 1513.66 2318.46 1513.66 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip520)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"112.177,1486.45 112.177,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip520)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"112.177,1254.65 131.075,1254.65 \"/>\n",
       "<polyline clip-path=\"url(#clip520)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"112.177,862.314 131.075,862.314 \"/>\n",
       "<polyline clip-path=\"url(#clip520)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"112.177,469.974 131.075,469.974 \"/>\n",
       "<polyline clip-path=\"url(#clip520)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"112.177,77.6339 131.075,77.6339 \"/>\n",
       "<path clip-path=\"url(#clip520)\" d=\"M56.2699 1268 L63.9087 1268 L63.9087 1241.63 L55.5986 1243.3 L55.5986 1239.04 L63.8624 1237.37 L68.5383 1237.37 L68.5383 1268 L76.1772 1268 L76.1772 1271.93 L56.2699 1271.93 L56.2699 1268 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M59.8578 875.659 L76.1772 875.659 L76.1772 879.594 L54.2328 879.594 L54.2328 875.659 Q56.8949 872.904 61.4782 868.275 Q66.0846 863.622 67.2652 862.279 Q69.5105 859.756 70.3902 858.02 Q71.2929 856.261 71.2929 854.571 Q71.2929 851.817 69.3485 850.08 Q67.4272 848.344 64.3254 848.344 Q62.1263 848.344 59.6726 849.108 Q57.2421 849.872 54.4643 851.423 L54.4643 846.701 Q57.2884 845.567 59.7421 844.988 Q62.1958 844.409 64.2328 844.409 Q69.6031 844.409 72.7976 847.094 Q75.992 849.78 75.992 854.27 Q75.992 856.4 75.1818 858.321 Q74.3948 860.219 72.2883 862.812 Q71.7096 863.483 68.6078 866.701 Q65.5059 869.895 59.8578 875.659 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M69.0476 468.62 Q72.404 469.337 74.279 471.606 Q76.1772 473.874 76.1772 477.208 Q76.1772 482.324 72.6587 485.124 Q69.1402 487.925 62.6587 487.925 Q60.4828 487.925 58.168 487.486 Q55.8764 487.069 53.4227 486.212 L53.4227 481.699 Q55.3671 482.833 57.6819 483.411 Q59.9967 483.99 62.5198 483.99 Q66.918 483.99 69.2096 482.254 Q71.5244 480.518 71.5244 477.208 Q71.5244 474.152 69.3717 472.439 Q67.242 470.703 63.4226 470.703 L59.3949 470.703 L59.3949 466.861 L63.6078 466.861 Q67.0569 466.861 68.8855 465.495 Q70.7142 464.106 70.7142 461.513 Q70.7142 458.851 68.8161 457.439 Q66.9411 456.004 63.4226 456.004 Q61.5013 456.004 59.3023 456.421 Q57.1032 456.838 54.4643 457.717 L54.4643 453.551 Q57.1264 452.81 59.4412 452.439 Q61.7791 452.069 63.8393 452.069 Q69.1633 452.069 72.2652 454.5 Q75.367 456.907 75.367 461.027 Q75.367 463.898 73.7235 465.888 Q72.08 467.856 69.0476 468.62 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M66.5939 64.4279 L54.7884 82.8769 L66.5939 82.8769 L66.5939 64.4279 M65.367 60.3539 L71.2466 60.3539 L71.2466 82.8769 L76.1772 82.8769 L76.1772 86.7658 L71.2466 86.7658 L71.2466 94.9139 L66.5939 94.9139 L66.5939 86.7658 L50.9921 86.7658 L50.9921 82.2519 L65.367 60.3539 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip522)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"175.59,87.9763 196.941,145.254 218.292,186.624 239.643,357.325 260.994,466.141 282.345,511.536 303.696,513.734 325.047,532.265 346.398,536.63 367.749,540.979 389.1,574.958 410.451,569.68 431.802,576.796 453.153,577.897 474.504,590.422 495.855,607.376 517.207,594.107 538.558,641.922 559.909,652.855 581.26,667.586 602.611,618.731 623.962,662.64 645.313,660.436 666.664,696.494 688.015,700.231 709.366,721.219 730.717,712.671 752.068,804.91 773.419,734.06 794.77,691.924 816.121,795.297 837.472,726.543 858.823,815.714 880.174,776.707 901.525,800.257 922.876,863.532 944.227,829.826 965.578,844.151 986.93,855.005 1008.28,965.497 1029.63,988.966 1050.98,960.311 1072.33,911.527 1093.68,1043.36 1115.04,965.644 1136.39,1045.05 1157.74,1163 1179.09,1065.65 1200.44,972.943 1221.79,1067.3 1243.14,1073.53 1264.49,1145.48 1285.84,1205.93 1307.2,1103.86 1328.55,1035.75 1349.9,1148.17 1371.25,1153.57 1392.6,1164.59 1413.95,1219.13 1435.3,1144.8 1456.65,1213.99 1478,1224.45 1499.35,1108.12 1520.71,982.074 1542.06,1111.99 1563.41,1124.9 1584.76,1286.71 1606.11,1154.85 1627.46,1272.54 1648.81,1319.17 1670.16,1209.86 1691.51,1230.22 1712.87,1356.03 1734.22,1343.35 1755.57,1309.29 1776.92,1353.7 1798.27,1353.99 1819.62,1293.47 1840.97,1237.23 1862.32,1322.42 1883.67,1402.98 1905.02,1243.64 1926.38,1307.57 1947.73,1196.8 1969.08,1344.15 1990.43,1445.72 2011.78,1197.36 2033.13,1372.91 2054.48,1308.28 2075.83,1246.94 2097.18,1255.83 2118.53,1319.23 2139.89,1368.13 2161.24,1333.64 2182.59,1307.77 2203.94,1359.54 2225.29,1338.28 2246.64,1339.67 2267.99,1364.23 2289.34,1300.6 \"/>\n",
       "<polyline clip-path=\"url(#clip522)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"175.59,434.262 196.941,443.538 218.292,193.28 239.643,457.815 260.994,441.87 282.345,442.45 303.696,445.458 325.047,452.826 346.398,471.58 367.749,505.106 389.1,409.728 410.451,508.643 431.802,481.557 453.153,467.304 474.504,465.881 495.855,465.662 517.207,465.679 538.558,471.755 559.909,479.374 581.26,482.779 602.611,479.745 623.962,483.562 645.313,495.432 666.664,515.036 688.015,521.544 709.366,505.544 730.717,522.781 752.068,521.86 773.419,514.536 794.77,514.407 816.121,525.948 837.472,518.436 858.823,526.64 880.174,515.059 901.525,525.847 922.876,523.333 944.227,526.53 965.578,523.321 986.93,512.698 1008.28,522.729 1029.63,518.82 1050.98,523.293 1072.33,510.997 1093.68,524.62 1115.04,523.009 1136.39,522.406 1157.74,520.885 1179.09,524.648 1200.44,524.999 1221.79,523.683 1243.14,517.961 1264.49,522.797 1285.84,537.869 1307.2,520.573 1328.55,515.908 1349.9,525.438 1371.25,510.962 1392.6,523.203 1413.95,528.105 1435.3,516.896 1456.65,517.213 1478,530.146 1499.35,523.821 1520.71,509.623 1542.06,509.809 1563.41,519.579 1584.76,524.588 1606.11,523.886 1627.46,523.789 1648.81,526.23 1670.16,519.555 1691.51,513.631 1712.87,531.587 1734.22,516.411 1755.57,520.809 1776.92,528.535 1798.27,519.264 1819.62,530.552 1840.97,507.84 1862.32,517.883 1883.67,526.389 1905.02,517.809 1926.38,525.512 1947.73,517.962 1969.08,517.01 1990.43,519.791 2011.78,516.063 2033.13,514.863 2054.48,508.905 2075.83,522.682 2097.18,531.22 2118.53,521.78 2139.89,519.955 2161.24,518.605 2182.59,536.067 2203.94,539.963 2225.29,517.277 2246.64,526.469 2267.99,520.274 2289.34,532.912 \"/>\n",
       "<path clip-path=\"url(#clip520)\" d=\"M1729.94 250.738 L2278.07 250.738 L2278.07 95.2176 L1729.94 95.2176  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip520)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1729.94,250.738 2278.07,250.738 2278.07,95.2176 1729.94,95.2176 1729.94,250.738 \"/>\n",
       "<polyline clip-path=\"url(#clip520)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1754.84,147.058 1904.21,147.058 \"/>\n",
       "<path clip-path=\"url(#clip520)\" d=\"M1929.1 138.412 L1933.36 138.412 L1933.36 164.338 L1929.1 164.338 L1929.1 138.412 M1929.1 128.319 L1933.36 128.319 L1933.36 133.713 L1929.1 133.713 L1929.1 128.319 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M1962.46 143.389 Q1964.06 140.518 1966.28 139.153 Q1968.5 137.787 1971.51 137.787 Q1975.56 137.787 1977.76 140.634 Q1979.96 143.458 1979.96 148.689 L1979.96 164.338 L1975.68 164.338 L1975.68 148.828 Q1975.68 145.102 1974.36 143.296 Q1973.04 141.49 1970.33 141.49 Q1967.02 141.49 1965.1 143.69 Q1963.18 145.889 1963.18 149.685 L1963.18 164.338 L1958.89 164.338 L1958.89 148.828 Q1958.89 145.078 1957.57 143.296 Q1956.25 141.49 1953.5 141.49 Q1950.24 141.49 1948.32 143.713 Q1946.39 145.912 1946.39 149.685 L1946.39 164.338 L1942.11 164.338 L1942.11 138.412 L1946.39 138.412 L1946.39 142.44 Q1947.85 140.055 1949.89 138.921 Q1951.93 137.787 1954.73 137.787 Q1957.55 137.787 1959.52 139.222 Q1961.51 140.657 1962.46 143.389 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M1992.57 160.449 L1992.57 174.199 L1988.29 174.199 L1988.29 138.412 L1992.57 138.412 L1992.57 142.347 Q1993.92 140.032 1995.95 138.921 Q1998.01 137.787 2000.86 137.787 Q2005.58 137.787 2008.52 141.537 Q2011.49 145.287 2011.49 151.398 Q2011.49 157.509 2008.52 161.259 Q2005.58 165.009 2000.86 165.009 Q1998.01 165.009 1995.95 163.898 Q1993.92 162.763 1992.57 160.449 M2007.06 151.398 Q2007.06 146.699 2005.12 144.037 Q2003.2 141.352 1999.82 141.352 Q1996.44 141.352 1994.5 144.037 Q1992.57 146.699 1992.57 151.398 Q1992.57 156.097 1994.5 158.782 Q1996.44 161.444 1999.82 161.444 Q2003.2 161.444 2005.12 158.782 Q2007.06 156.097 2007.06 151.398 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2033.57 142.393 Q2032.85 141.977 2032 141.791 Q2031.16 141.583 2030.14 141.583 Q2026.53 141.583 2024.59 143.944 Q2022.67 146.282 2022.67 150.68 L2022.67 164.338 L2018.38 164.338 L2018.38 138.412 L2022.67 138.412 L2022.67 142.44 Q2024.01 140.078 2026.16 138.944 Q2028.31 137.787 2031.39 137.787 Q2031.83 137.787 2032.37 137.856 Q2032.9 137.903 2033.55 138.018 L2033.57 142.393 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2047.04 141.398 Q2043.62 141.398 2041.62 144.083 Q2039.63 146.745 2039.63 151.398 Q2039.63 156.051 2041.6 158.736 Q2043.59 161.398 2047.04 161.398 Q2050.44 161.398 2052.43 158.713 Q2054.43 156.027 2054.43 151.398 Q2054.43 146.791 2052.43 144.106 Q2050.44 141.398 2047.04 141.398 M2047.04 137.787 Q2052.6 137.787 2055.77 141.398 Q2058.94 145.009 2058.94 151.398 Q2058.94 157.764 2055.77 161.398 Q2052.6 165.009 2047.04 165.009 Q2041.46 165.009 2038.29 161.398 Q2035.14 157.764 2035.14 151.398 Q2035.14 145.009 2038.29 141.398 Q2041.46 137.787 2047.04 137.787 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2062.94 138.412 L2067.46 138.412 L2075.56 160.171 L2083.66 138.412 L2088.18 138.412 L2078.45 164.338 L2072.67 164.338 L2062.94 138.412 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2116.23 150.31 L2116.23 152.393 L2096.65 152.393 Q2096.93 156.791 2099.29 159.106 Q2101.67 161.398 2105.91 161.398 Q2108.36 161.398 2110.65 160.796 Q2112.97 160.194 2115.24 158.99 L2115.24 163.018 Q2112.94 163.99 2110.54 164.5 Q2108.13 165.009 2105.65 165.009 Q2099.45 165.009 2095.81 161.398 Q2092.2 157.787 2092.2 151.629 Q2092.2 145.264 2095.63 141.537 Q2099.08 137.787 2104.91 137.787 Q2110.14 137.787 2113.18 141.166 Q2116.23 144.523 2116.23 150.31 M2111.97 149.06 Q2111.93 145.565 2110 143.481 Q2108.11 141.398 2104.96 141.398 Q2101.39 141.398 2099.24 143.412 Q2097.11 145.426 2096.79 149.083 L2111.97 149.06 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2140.28 142.347 L2140.28 128.319 L2144.54 128.319 L2144.54 164.338 L2140.28 164.338 L2140.28 160.449 Q2138.94 162.763 2136.88 163.898 Q2134.84 165.009 2131.97 165.009 Q2127.27 165.009 2124.31 161.259 Q2121.37 157.509 2121.37 151.398 Q2121.37 145.287 2124.31 141.537 Q2127.27 137.787 2131.97 137.787 Q2134.84 137.787 2136.88 138.921 Q2138.94 140.032 2140.28 142.347 M2125.77 151.398 Q2125.77 156.097 2127.69 158.782 Q2129.63 161.444 2133.01 161.444 Q2136.39 161.444 2138.34 158.782 Q2140.28 156.097 2140.28 151.398 Q2140.28 146.699 2138.34 144.037 Q2136.39 141.352 2133.01 141.352 Q2129.63 141.352 2127.69 144.037 Q2125.77 146.699 2125.77 151.398 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2168.38 128.319 L2172.64 128.319 L2172.64 164.338 L2168.38 164.338 L2168.38 128.319 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2191.6 141.398 Q2188.17 141.398 2186.18 144.083 Q2184.19 146.745 2184.19 151.398 Q2184.19 156.051 2186.16 158.736 Q2188.15 161.398 2191.6 161.398 Q2195 161.398 2196.99 158.713 Q2198.99 156.027 2198.99 151.398 Q2198.99 146.791 2196.99 144.106 Q2195 141.398 2191.6 141.398 M2191.6 137.787 Q2197.16 137.787 2200.33 141.398 Q2203.5 145.009 2203.5 151.398 Q2203.5 157.764 2200.33 161.398 Q2197.16 165.009 2191.6 165.009 Q2186.02 165.009 2182.85 161.398 Q2179.7 157.764 2179.7 151.398 Q2179.7 145.009 2182.85 141.398 Q2186.02 137.787 2191.6 137.787 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2227.09 139.176 L2227.09 143.203 Q2225.28 142.277 2223.34 141.815 Q2221.39 141.352 2219.31 141.352 Q2216.14 141.352 2214.54 142.324 Q2212.97 143.296 2212.97 145.24 Q2212.97 146.722 2214.1 147.578 Q2215.24 148.412 2218.66 149.176 L2220.12 149.5 Q2224.66 150.472 2226.55 152.254 Q2228.48 154.014 2228.48 157.185 Q2228.48 160.796 2225.61 162.902 Q2222.76 165.009 2217.76 165.009 Q2215.67 165.009 2213.41 164.592 Q2211.16 164.199 2208.66 163.388 L2208.66 158.99 Q2211.02 160.217 2213.31 160.842 Q2215.61 161.444 2217.85 161.444 Q2220.86 161.444 2222.48 160.426 Q2224.1 159.384 2224.1 157.509 Q2224.1 155.773 2222.92 154.847 Q2221.76 153.921 2217.8 153.064 L2216.32 152.717 Q2212.36 151.884 2210.61 150.171 Q2208.85 148.435 2208.85 145.426 Q2208.85 141.768 2211.44 139.778 Q2214.03 137.787 2218.8 137.787 Q2221.16 137.787 2223.24 138.134 Q2225.33 138.481 2227.09 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2251.79 139.176 L2251.79 143.203 Q2249.98 142.277 2248.04 141.815 Q2246.09 141.352 2244.01 141.352 Q2240.84 141.352 2239.24 142.324 Q2237.67 143.296 2237.67 145.24 Q2237.67 146.722 2238.8 147.578 Q2239.93 148.412 2243.36 149.176 L2244.82 149.5 Q2249.36 150.472 2251.25 152.254 Q2253.17 154.014 2253.17 157.185 Q2253.17 160.796 2250.3 162.902 Q2247.46 165.009 2242.46 165.009 Q2240.37 165.009 2238.11 164.592 Q2235.86 164.199 2233.36 163.388 L2233.36 158.99 Q2235.72 160.217 2238.01 160.842 Q2240.3 161.444 2242.55 161.444 Q2245.56 161.444 2247.18 160.426 Q2248.8 159.384 2248.8 157.509 Q2248.8 155.773 2247.62 154.847 Q2246.46 153.921 2242.5 153.064 L2241.02 152.717 Q2237.06 151.884 2235.3 150.171 Q2233.55 148.435 2233.55 145.426 Q2233.55 141.768 2236.14 139.778 Q2238.73 137.787 2243.5 137.787 Q2245.86 137.787 2247.94 138.134 Q2250.03 138.481 2251.79 139.176 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip520)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1754.84,198.898 1904.21,198.898 \"/>\n",
       "<path clip-path=\"url(#clip520)\" d=\"M1936.51 182.891 L1936.51 190.252 L1945.28 190.252 L1945.28 193.562 L1936.51 193.562 L1936.51 207.636 Q1936.51 210.807 1937.37 211.71 Q1938.25 212.613 1940.91 212.613 L1945.28 212.613 L1945.28 216.178 L1940.91 216.178 Q1935.98 216.178 1934.1 214.349 Q1932.23 212.497 1932.23 207.636 L1932.23 193.562 L1929.1 193.562 L1929.1 190.252 L1932.23 190.252 L1932.23 182.891 L1936.51 182.891 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M1960.93 193.238 Q1957.5 193.238 1955.51 195.923 Q1953.52 198.585 1953.52 203.238 Q1953.52 207.891 1955.49 210.576 Q1957.48 213.238 1960.93 213.238 Q1964.33 213.238 1966.32 210.553 Q1968.32 207.867 1968.32 203.238 Q1968.32 198.631 1966.32 195.946 Q1964.33 193.238 1960.93 193.238 M1960.93 189.627 Q1966.49 189.627 1969.66 193.238 Q1972.83 196.849 1972.83 203.238 Q1972.83 209.604 1969.66 213.238 Q1966.49 216.849 1960.93 216.849 Q1955.35 216.849 1952.18 213.238 Q1949.03 209.604 1949.03 203.238 Q1949.03 196.849 1952.18 193.238 Q1955.35 189.627 1960.93 189.627 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M1990.68 218.585 Q1988.87 223.215 1987.16 224.627 Q1985.44 226.039 1982.57 226.039 L1979.17 226.039 L1979.17 222.474 L1981.67 222.474 Q1983.43 222.474 1984.4 221.64 Q1985.38 220.807 1986.56 217.705 L1987.32 215.761 L1976.83 190.252 L1981.35 190.252 L1989.45 210.529 L1997.55 190.252 L2002.06 190.252 L1990.68 218.585 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2023.01 180.159 L2027.27 180.159 L2027.27 216.178 L2023.01 216.178 L2023.01 180.159 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2046.23 193.238 Q2042.81 193.238 2040.81 195.923 Q2038.82 198.585 2038.82 203.238 Q2038.82 207.891 2040.79 210.576 Q2042.78 213.238 2046.23 213.238 Q2049.63 213.238 2051.62 210.553 Q2053.62 207.867 2053.62 203.238 Q2053.62 198.631 2051.62 195.946 Q2049.63 193.238 2046.23 193.238 M2046.23 189.627 Q2051.79 189.627 2054.96 193.238 Q2058.13 196.849 2058.13 203.238 Q2058.13 209.604 2054.96 213.238 Q2051.79 216.849 2046.23 216.849 Q2040.65 216.849 2037.48 213.238 Q2034.33 209.604 2034.33 203.238 Q2034.33 196.849 2037.48 193.238 Q2040.65 189.627 2046.23 189.627 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2081.72 191.016 L2081.72 195.043 Q2079.91 194.117 2077.97 193.655 Q2076.02 193.192 2073.94 193.192 Q2070.77 193.192 2069.17 194.164 Q2067.6 195.136 2067.6 197.08 Q2067.6 198.562 2068.73 199.418 Q2069.87 200.252 2073.29 201.016 L2074.75 201.34 Q2079.29 202.312 2081.18 204.094 Q2083.11 205.854 2083.11 209.025 Q2083.11 212.636 2080.24 214.742 Q2077.39 216.849 2072.39 216.849 Q2070.31 216.849 2068.04 216.432 Q2065.79 216.039 2063.29 215.228 L2063.29 210.83 Q2065.65 212.057 2067.94 212.682 Q2070.24 213.284 2072.48 213.284 Q2075.49 213.284 2077.11 212.266 Q2078.73 211.224 2078.73 209.349 Q2078.73 207.613 2077.55 206.687 Q2076.39 205.761 2072.43 204.904 L2070.95 204.557 Q2067 203.724 2065.24 202.011 Q2063.48 200.275 2063.48 197.266 Q2063.48 193.608 2066.07 191.618 Q2068.66 189.627 2073.43 189.627 Q2075.79 189.627 2077.87 189.974 Q2079.96 190.321 2081.72 191.016 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip520)\" d=\"M2106.42 191.016 L2106.42 195.043 Q2104.61 194.117 2102.67 193.655 Q2100.72 193.192 2098.64 193.192 Q2095.47 193.192 2093.87 194.164 Q2092.3 195.136 2092.3 197.08 Q2092.3 198.562 2093.43 199.418 Q2094.56 200.252 2097.99 201.016 L2099.45 201.34 Q2103.99 202.312 2105.88 204.094 Q2107.81 205.854 2107.81 209.025 Q2107.81 212.636 2104.93 214.742 Q2102.09 216.849 2097.09 216.849 Q2095 216.849 2092.74 216.432 Q2090.49 216.039 2087.99 215.228 L2087.99 210.83 Q2090.35 212.057 2092.64 212.682 Q2094.93 213.284 2097.18 213.284 Q2100.19 213.284 2101.81 212.266 Q2103.43 211.224 2103.43 209.349 Q2103.43 207.613 2102.25 206.687 Q2101.09 205.761 2097.13 204.904 L2095.65 204.557 Q2091.69 203.724 2089.93 202.011 Q2088.18 200.275 2088.18 197.266 Q2088.18 193.608 2090.77 191.618 Q2093.36 189.627 2098.13 189.627 Q2100.49 189.627 2102.57 189.974 Q2104.66 190.321 2106.42 191.016 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The loss of the improved model seems to decrease a lot quicker! A good sign\n",
    "using Plots\n",
    "plot(improved_loss, label=\"improved loss\")\n",
    "plot!(toy_loss, label=\"toy loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22×1×1 Array{Float32, 3}:\n",
       "[:, :, 1] =\n",
       " -0.7675283\n",
       " -1.2492194\n",
       " -0.81128615\n",
       " -1.9402307\n",
       "  4.681537\n",
       " -1.317524\n",
       " -0.5782627\n",
       " -0.52665794\n",
       " -1.3640952\n",
       " -1.3325182\n",
       "  0.19883598\n",
       "  0.42779666\n",
       " -0.109821245\n",
       " -1.4178876\n",
       "  4.8161964\n",
       " -0.8528526\n",
       " -4.0018697\n",
       " -1.0516504\n",
       "  1.3994135\n",
       " -0.17169364\n",
       " -1.2398024\n",
       "  1.210072"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I pass just a beginning-of-sequence token to our model and it predicts what it thinks that the first token should be\n",
    "# We will however have to normalise these outputs using a softmax function to get the actual probabilities\n",
    "x = Flux.onehotbatch(\"#\",alphabet)\n",
    "x = reshape(x,(22,1,1))\n",
    "logits = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- : 0.18721215\n",
      "A : 0.11564805\n",
      "C : 0.17919686\n",
      "D : 0.057947654\n",
      "E : 43.534462\n",
      "F : 0.1080125\n",
      "G : 0.22622004\n",
      "H : 0.23820053\n",
      "I : 0.10309756\n",
      "K : 0.106405035\n",
      "L : 0.49206302\n",
      "M : 0.61866736\n",
      "N : 0.36138707\n",
      "P : 0.097698204\n",
      "Q : 49.80982\n",
      "R : 0.17190088\n",
      "S : 0.007373568\n",
      "T : 0.14090987\n",
      "V : 1.6346508\n",
      "W : 0.33970484\n",
      "Y : 0.11674226\n",
      "# : 1.3526798\n"
     ]
    }
   ],
   "source": [
    "# Here are the actual probabilities\n",
    "for (letter,prob) in zip(alphabet,softmax(logits))\n",
    "    if true || prob >= 0.1\n",
    "        println(letter, \" : \", prob)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have a very simple greedy sampling loop, which simply starts from a #-token and then just iteratively adds new tokens for 130 steps\n",
    "for i in 1:130\n",
    "    x *= Flux.onecold(model(reshape(onehotbatch(x, alphabet), (vocab_size, length(x), 1))), alphabet)[end]\n",
    "end\n",
    "# Note that this is something you would want to optimise more in an actual model and you also in general won't just sample greedily but will instead sample\n",
    "# from the distribution. It is however normal to weight this distribution using something known as temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#QQQLQESGPGVVQPSLRSLSLTCAASGFTFSYGYWISQAPPGKGRWIGWIGNYYSRQGNTTSVKSRQGTISRDTSKNTLSVYLSSVTAAADTAVYCAQNYYYYYYYYYWGWGWGTVTVTVS---------\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I ran two models which had trained on 100 and 500 batches respectively. The number in paranthesis is the identity to the closest real *germline* sequence\n",
    "# 100 batches: QQQLQESGPGVVQPSLRSLSLTCAASGFTFSYGYWISQAPPGKGRWIGWIGNYYSRQGNTTSVKSRQGTISRDTSKNTLSVYLSSVTAAADTAVYCAQNYYYYYYYYYWGWGWGTVTVTVS (54%)\n",
    "# 500 batches: QVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLEWMGWINPNSGGTDYAQKFQGRVTMTRDTSISTAYMEVSSLSSDDTAVYYCARDTRGDYSYYYDAIWGTLVWGQGQGTVSS (83%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Above we've created our own transformer layers which is sometimes necessary/nice to do, but you can also use this one from the Onion.jl package (or alternatives in Transformers.jl)\n",
    "TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using that, we can really reduce our transformer to only this!:\n",
    "struct OnionModel\n",
    "    vocab_embed::Dense\n",
    "    layers::Vector{TransformerBlock}\n",
    "    final::Chain\n",
    "end\n",
    "\n",
    "Flux.@layer OnionModel\n",
    "\n",
    "function OnionModel(vocab_size::Int, dim::Int, n_layers::Int, n_heads::Int)\n",
    "    vocab_embed = Dense(vocab_size, dim, bias=false)\n",
    "    layers = [TransformerBlock(dim, n_heads) for _ in 1:n_layers]\n",
    "    final = Chain(\n",
    "        RMSNorm(dim),\n",
    "        Dense(dim, vocab_size)\n",
    "    )\n",
    "    return OnionModel(vocab_embed, layers, final)\n",
    "end\n",
    "\n",
    "function (model::OnionModel)(x::AbstractArray)\n",
    "    x = model.vocab_embed(x)\n",
    "    h = randn(Float32, dim, size(x,2), 1)\n",
    "    rope = RoPE(dim ÷ n_heads, size(x,2))\n",
    "    mask = Onion.causal_mask(h)\n",
    "    for layer in model.layers\n",
    "        x = checkpointed(layer, x, 1, rope, mask)\n",
    "    end\n",
    "    return model.final(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.1f0\n",
    "layers = 8\n",
    "dim = 128\n",
    "n_heads = 2\n",
    " \n",
    "model = OnionModel(vocab_size, dim, layers, n_heads)\n",
    "base_lr = 0.001f0\n",
    "rule = Optimisers.Adam(base_lr) \n",
    "opt_state = Optimisers.setup(rule, model)\n",
    "\n",
    "model = train_ar_model(model, batches[1:500], opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hope you enjoyed, good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
